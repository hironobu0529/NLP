{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb106bf-2dab-4a0f-b469-07991d5d549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"df_p1.csv\")\n",
    "# 先頭2列を削除\n",
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b47dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Part</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>site</th>\n",
       "      <th>Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>partition_P1</th>\n",
       "      <th>senti_sim_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>PANSS_misorientation</th>\n",
       "      <th>PANSS_willness</th>\n",
       "      <th>PANSS_impulsivity</th>\n",
       "      <th>PANSS_absorb</th>\n",
       "      <th>PANSS_escape</th>\n",
       "      <th>PANSSpositive</th>\n",
       "      <th>PANSSnegative</th>\n",
       "      <th>PANSSpathology</th>\n",
       "      <th>PANSScomposition</th>\n",
       "      <th>PANSStotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UKM153</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>K</td>\n",
       "      <td>20210428</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.542984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UKM173</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>K</td>\n",
       "      <td>20211221</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.640910</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UAM003</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>20200828</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.669861</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UTM005</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>T</td>\n",
       "      <td>20191224</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.637091</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UAM009</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>20191107</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.568720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Visit  Part  diagnosis site      Date  Age  Gender partition_P1  \\\n",
       "0  UKM153      1     1        0.0    K  20210428   50       2        Train   \n",
       "1  UKM173      4     1        0.0    K  20211221   27       2        Train   \n",
       "2  UAM003      4     1        1.0    A  20200828   30       1        Train   \n",
       "3  UTM005      5     1        1.0    T  20191224   52       1        Train   \n",
       "4  UAM009      3     1        1.0    A  20191107   36       1         Test   \n",
       "\n",
       "   senti_sim_mean  ...  PANSS_misorientation  PANSS_willness  \\\n",
       "0        0.542984  ...                   1.0             1.0   \n",
       "1        0.640910  ...                   1.0             1.0   \n",
       "2        0.669861  ...                   1.0             1.0   \n",
       "3        0.637091  ...                   3.0             3.0   \n",
       "4        0.568720  ...                   1.0             3.0   \n",
       "\n",
       "   PANSS_impulsivity  PANSS_absorb  PANSS_escape  PANSSpositive  \\\n",
       "0                1.0           1.0           1.0            8.0   \n",
       "1                1.0           1.0           1.0            7.0   \n",
       "2                1.0           1.0           1.0           11.0   \n",
       "3                1.0           2.0           4.0           13.0   \n",
       "4                1.0           1.0           2.0           16.0   \n",
       "\n",
       "   PANSSnegative  PANSSpathology  PANSScomposition  PANSStotal  \n",
       "0            7.0            16.0               1.0        31.0  \n",
       "1            7.0            16.0               0.0        30.0  \n",
       "2           12.0            21.0              -1.0        44.0  \n",
       "3           26.0            34.0             -13.0        73.0  \n",
       "4           12.0            44.0               4.0        72.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe06f10-cc18-438a-9ab6-bc5bf5ab7506",
   "metadata": {},
   "source": [
    "# データ前処理\n",
    "#1 変数の固定 done\n",
    "#2 欠損値の処理\n",
    "#3 標準化\n",
    "#4 歪度の処理\n",
    "#5 交絡先行残差化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7845b12-1190-48c2-a014-675bbf37847f",
   "metadata": {},
   "source": [
    "#1 変数の確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b9307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Visit',\n",
       " 'Part',\n",
       " 'diagnosis',\n",
       " 'site',\n",
       " 'Date',\n",
       " 'Age',\n",
       " 'Gender',\n",
       " 'partition_P1',\n",
       " 'senti_sim_mean',\n",
       " 'senti_sim_var',\n",
       " 'w2V_adj_mean',\n",
       " 'w2V_adj_variance',\n",
       " 'num_morhpheme',\n",
       " 'avg_sentence_length',\n",
       " 'avg_word_length',\n",
       " 'num_sentences',\n",
       " 'redundancy',\n",
       " 'w2Vsim_mean',\n",
       " 'w2Vsim_variance',\n",
       " 'words_nodes_ratio',\n",
       " 'cluster_coefficient',\n",
       " 'average_closeness_centrality',\n",
       " 'average_distance',\n",
       " 'diameter',\n",
       " 'density',\n",
       " 'senti_sim_mean_rrb',\n",
       " 'senti_sim_var_rrb',\n",
       " 'max_nesting_relation_CEJCminus',\n",
       " 'max_nesting_depth_CEJCminus',\n",
       " 'max_tree_height_CEJCminus',\n",
       " 'perseveration_ratio',\n",
       " 'duplicates_adj_ratio',\n",
       " 'unique_num_adj_ratio',\n",
       " 'num_NOUN_ratio',\n",
       " 'num_PROPN_ratio',\n",
       " 'num_VERB_ratio',\n",
       " 'num_ADJ_ratio',\n",
       " 'num_ADV_ratio',\n",
       " 'num_INTJ_ratio',\n",
       " 'num_PRON_ratio',\n",
       " 'num_NUM_ratio',\n",
       " 'num_AUX_ratio',\n",
       " 'num_CCONJ_ratio',\n",
       " 'num_SCONJ_ratio',\n",
       " 'num_DET_ratio',\n",
       " 'num_ADP_ratio',\n",
       " 'num_PART_ratio',\n",
       " 'num_PUNCT_ratio',\n",
       " 'num_SYM_ratio',\n",
       " 'num_intj_general_ratio',\n",
       " 'num_intj_filler_ratio',\n",
       " 'num_kakujoshi_ratio',\n",
       " 'num_fukujoshi_ratio',\n",
       " 'num_kakarijoshi_ratio',\n",
       " 'num_setuzokujoshi_ratio',\n",
       " 'num_shuujoshi_ratio',\n",
       " 'num_rentaishi_ratio',\n",
       " 'num_settouji_ratio',\n",
       " 'num_setsubiji_ratio',\n",
       " 'num_person_ratio',\n",
       " 'num_god_ratio',\n",
       " 'num_place_ratio',\n",
       " 'num_time_ratio',\n",
       " 'acl_ratio',\n",
       " 'advcl_ratio',\n",
       " 'advmod_ratio',\n",
       " 'amod_ratio',\n",
       " 'aux_ratio',\n",
       " 'case_ratio',\n",
       " 'cc_ratio',\n",
       " 'ccomp_ratio',\n",
       " 'compound_ratio',\n",
       " 'cop_ratio',\n",
       " 'csubj_ratio',\n",
       " 'dep_ratio',\n",
       " 'det_ratio',\n",
       " 'discourse_ratio',\n",
       " 'fixed_ratio',\n",
       " 'mark_ratio',\n",
       " 'nmod_ratio',\n",
       " 'nsubj_ratio',\n",
       " 'nummod_ratio',\n",
       " 'obj_ratio',\n",
       " 'obl_ratio',\n",
       " 'reparandum_ratio',\n",
       " 'root_ratio',\n",
       " 'duplicates_ratio',\n",
       " 'unique_num_ratio',\n",
       " 'number_of_nodes_ratio',\n",
       " 'perseveration_rrb_ratio',\n",
       " 'total_nesting_depth_CEJCminus_ratio',\n",
       " 'total_num_nodes_CEJCminus_ratio',\n",
       " 'total_num_leaves_CEJCminus_ratio',\n",
       " 'total_distance_CEJCminus_ratio',\n",
       " 'total_tree_height_CEJCminus_ratio',\n",
       " 'PANSS_delusion',\n",
       " 'PANSS_integration',\n",
       " 'PANSS_hallucination',\n",
       " 'PANSS_excitement',\n",
       " 'PANSS_grandiosity',\n",
       " 'PANSS_suspicion',\n",
       " 'PANSS_hostility',\n",
       " 'PANSS_blunted_affect',\n",
       " 'PANSS_autism',\n",
       " 'PANSS_communication',\n",
       " 'PANSS_motivation',\n",
       " 'PANSS_abstraction',\n",
       " 'PANSS_fluency',\n",
       " 'PANSS_repetitive',\n",
       " 'PANSS_psychosomatics',\n",
       " 'PANSS_anxiety',\n",
       " 'PANSS_guilty',\n",
       " 'PANSS_nervous',\n",
       " 'PANSS_unnatural',\n",
       " 'PANSS_depression',\n",
       " 'PANSS_slowness',\n",
       " 'PANSS_uncooperative',\n",
       " 'PANSS_unnatural_thoughts',\n",
       " 'PANSS_undirection',\n",
       " 'PANSS_attention',\n",
       " 'PANSS_misorientation',\n",
       " 'PANSS_willness',\n",
       " 'PANSS_impulsivity',\n",
       " 'PANSS_absorb',\n",
       " 'PANSS_escape',\n",
       " 'PANSSpositive',\n",
       " 'PANSSnegative',\n",
       " 'PANSSpathology',\n",
       " 'PANSScomposition',\n",
       " 'PANSStotal']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a605ce-61a8-4d3e-9e8c-9c9751c699c9",
   "metadata": {},
   "source": [
    "['appos', 'clf', 'conj', 'dislocated', 'expl', 'flat', 'goeswith', 'iobj', 'list', 'orphan', 'parataxis', 'punct', 'vocative', 'xcomp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031e65e-0b01-4922-a026-feb626eaf49c",
   "metadata": {},
   "source": [
    "# データ前処理\n",
    "#1 変数の固定 done\n",
    "#2 欠損値の処理\n",
    "#3 標準化\n",
    "#4 歪度の処理\n",
    "#5 交絡先行残差化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b347d-3e4f-42c3-a323-770ecc16a581",
   "metadata": {},
   "source": [
    "#1 変数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5240600b-bb11-4979-b812-f675aeabb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list=['senti_sim_mean',\n",
    " 'senti_sim_var',\n",
    " 'w2V_adj_mean',\n",
    " 'w2V_adj_variance',\n",
    " 'num_morhpheme',\n",
    " 'avg_sentence_length',\n",
    " 'avg_word_length',\n",
    " 'num_sentences',\n",
    " 'redundancy',\n",
    " 'w2Vsim_mean',\n",
    " 'w2Vsim_variance',\n",
    " 'cluster_coefficient',\n",
    " 'average_closeness_centrality',\n",
    " 'average_distance',\n",
    " 'diameter',\n",
    " 'density',\n",
    " 'senti_sim_mean_rrb',\n",
    " 'senti_sim_var_rrb',\n",
    " 'max_nesting_depth_CEJCminus',\n",
    " 'max_tree_height_CEJCminus',\n",
    " 'perseveration_ratio',\n",
    " 'duplicates_adj_ratio',\n",
    " 'num_NOUN_ratio',\n",
    " 'num_PROPN_ratio',\n",
    " 'num_VERB_ratio',\n",
    " 'num_ADJ_ratio',\n",
    " 'num_ADV_ratio',\n",
    " 'num_INTJ_ratio',\n",
    " 'num_PRON_ratio',\n",
    " 'num_NUM_ratio',\n",
    " 'num_AUX_ratio',\n",
    " 'num_CCONJ_ratio',\n",
    " 'num_SCONJ_ratio',\n",
    " 'num_DET_ratio',\n",
    " 'num_ADP_ratio',\n",
    " 'num_PART_ratio',\n",
    " 'num_PUNCT_ratio',\n",
    " 'num_SYM_ratio',\n",
    " 'num_intj_general_ratio',\n",
    " 'num_intj_filler_ratio',\n",
    " 'num_kakujoshi_ratio',\n",
    " 'num_fukujoshi_ratio',\n",
    " 'num_kakarijoshi_ratio',\n",
    " 'num_setuzokujoshi_ratio',\n",
    " 'num_shuujoshi_ratio',\n",
    " 'num_rentaishi_ratio',\n",
    " 'num_settouji_ratio',\n",
    " 'num_setsubiji_ratio',\n",
    " 'num_person_ratio',\n",
    " 'num_god_ratio',\n",
    " 'num_place_ratio',\n",
    " 'num_time_ratio',\n",
    " 'acl_ratio',\n",
    " 'advcl_ratio',\n",
    " 'advmod_ratio',\n",
    " 'amod_ratio',\n",
    " 'case_ratio',\n",
    " 'cc_ratio',\n",
    " 'ccomp_ratio',\n",
    " 'compound_ratio',\n",
    " 'csubj_ratio',\n",
    " 'det_ratio',\n",
    " 'discourse_ratio',\n",
    " 'mark_ratio',\n",
    " 'nmod_ratio',\n",
    " 'nsubj_ratio',\n",
    " 'nummod_ratio',\n",
    " 'obj_ratio',\n",
    " 'obl_ratio',\n",
    " 'reparandum_ratio',\n",
    " 'duplicates_ratio',\n",
    " 'unique_num_ratio',\n",
    " 'perseveration_rrb_ratio',\n",
    " 'total_nesting_depth_CEJCminus_ratio',\n",
    " 'total_distance_CEJCminus_ratio',\n",
    " 'total_tree_height_CEJCminus_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5e219b-6a25-4de1-85cd-0700cd500b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(var_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be66c496-700a-4564-bed2-31ab5a27ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_listを数値化（変換できない値はNaN）\n",
    "df[var_list] = df[var_list].apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a2cf8-0a13-41d5-87b7-ef97b647241b",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc97630-8743-41e9-9a26-77f7af08e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] NaN=0, Inf=0\n",
      "[Test] NaN=0, Inf=0\n",
      "\n",
      "[参考] covariate分布差（前処理後の列そのまま）\n",
      "- Sex Train:\n",
      "Gender\n",
      "1    0.498623\n",
      "2    0.501377\n",
      "- Sex Test :\n",
      "Gender\n",
      "1    0.482353\n",
      "2    0.517647\n",
      "- Site Train:\n",
      "site\n",
      "A    0.071625\n",
      "B    0.035813\n",
      "G    0.107438\n",
      "K    0.542700\n",
      "M    0.049587\n",
      "N    0.019284\n",
      "O    0.104683\n",
      "T    0.068871\n",
      "- Site Test :\n",
      "site\n",
      "A    0.164706\n",
      "G    0.058824\n",
      "K    0.623529\n",
      "M    0.047059\n",
      "O    0.105882\n",
      "- Age mean±sd (Train)=49.53±15.67, (Test)=50.01±15.49\n",
      "==== Safety Check: OK ====\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========= 既定カラム名 =========\n",
    "ID_COL    = \"ID\"\n",
    "AGE_COL   = \"Age\"\n",
    "SEX_COL   = \"Gender\"\n",
    "SITE_COL  = \"site\"          # 参照用途のみ\n",
    "DIAG_COL=\"diagnosis\"\n",
    "PART_COL  = \"Part\"\n",
    "PARTITION = \"partition_P1\"  # Part1 の固定パーティション\n",
    "\n",
    "# ========= （参考）site補完ユーティリティ（使用しなくても可） =========\n",
    "def ensure_site(df, id_col=ID_COL, site_col=SITE_COL):\n",
    "    if site_col not in df.columns or df[site_col].isna().all():\n",
    "        df = df.copy()\n",
    "        df[site_col] = df[id_col].astype(str).str[1]\n",
    "    return df\n",
    "\n",
    "# ========= 欠測補完：ID内中央値→Train全体中央値 =========\n",
    "def impute_within_id_then_global(train, test, feat_cols, id_col=ID_COL):\n",
    "    train = train.copy(); test = test.copy()\n",
    "    train[feat_cols] = train[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test[feat_cols]  = test[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    id_meds  = train.groupby(id_col)[feat_cols].median()\n",
    "    glob_med = train[feat_cols].median()\n",
    "\n",
    "    bad_glob = glob_med.index[glob_med.isna()].tolist()\n",
    "    if bad_glob:\n",
    "        raise ValueError(\n",
    "            \"Trainで全欠損（全行NaN）のため補完不能な特徴があります。\"\n",
    "            f\" 手動で除外してください: {bad_glob[:15]}{' ...' if len(bad_glob)>15 else ''}\"\n",
    "        )\n",
    "\n",
    "    def _apply(block):\n",
    "        out = block.copy()\n",
    "        out = out.join(id_meds, on=id_col, rsuffix=\"_idmed\")\n",
    "        for c in feat_cols:\n",
    "            idmed = out[f\"{c}_idmed\"]\n",
    "            out[c] = out[c].where(~out[c].isna(), idmed)  # まずID内中央値\n",
    "            out[c] = out[c].fillna(glob_med[c])           # 残れば全体中央値\n",
    "            out.drop(columns=[f\"{c}_idmed\"], inplace=True)\n",
    "        return out\n",
    "\n",
    "    train_o = _apply(train)\n",
    "    test_o  = _apply(test)\n",
    "\n",
    "    miss_tr = train_o[feat_cols].isna().sum()\n",
    "    miss_te = test_o[feat_cols].isna().sum()\n",
    "    if miss_tr.sum() > 0 or miss_te.sum() > 0:\n",
    "        detail = {\n",
    "            \"train\": miss_tr[miss_tr>0].sort_values(ascending=False).to_dict(),\n",
    "            \"test\":  miss_te[miss_te>0].sort_values(ascending=False).to_dict(),\n",
    "        }\n",
    "        raise ValueError(f\"補完後にNaNが残存: {detail}\")\n",
    "\n",
    "    return train_o, test_o, {\"id_medians\": id_meds, \"global_median\": glob_med}\n",
    "\n",
    "# ========= Robust Scaling（Train fit → Train/Test apply） =========\n",
    "def robust_scale_fit(train, feat_cols):\n",
    "    med = train[feat_cols].median()\n",
    "    mad = (train[feat_cols] - med).abs().median()\n",
    "    scale = 1.4826 * mad\n",
    "    scale = scale.replace(0, np.nan).fillna(1.0)   # 0分散の保険\n",
    "    return {\"median\": med, \"scale\": scale}\n",
    "\n",
    "def robust_scale_apply(df, feat_cols, params):\n",
    "    med, scale = params[\"median\"], params[\"scale\"]\n",
    "    out = df.copy()\n",
    "    out[feat_cols] = (out[feat_cols] - med) / scale\n",
    "    return out\n",
    "\n",
    "# ========= ユーティリティ =========\n",
    "def _assert_no_nan(df, cols, tag):\n",
    "    n = int(df[cols].isna().sum().sum())\n",
    "    if n>0:\n",
    "        bad = df[cols].isna().sum()\n",
    "        bad = bad[bad>0].sort_values(ascending=False).head(12).to_dict()\n",
    "        raise AssertionError(f\"[{tag}] NaN総数={n}。例: {bad}\")\n",
    "\n",
    "# ========= メイン前処理（欠測補完 + 標準化のみ） =========\n",
    "def preprocess_part1_with_varlist(df, var_list):\n",
    "    \"\"\"\n",
    "    - partition_P1 で Train/Test に分割\n",
    "    - 各特徴を「ID内中央値 → Train全体中央値」で補完\n",
    "    - Trainの中央値/MADで Robust標準化（Testにも適用）\n",
    "    - 残差化や交絡調整は行わない\n",
    "    \"\"\"\n",
    "    # 必要列（Age/Gender/siteは保持するが、処理本体では使わない）\n",
    "    needed = [ID_COL, PARTITION, AGE_COL, SEX_COL, PART_COL]\n",
    "    miss = [c for c in needed if c not in df.columns]\n",
    "    if miss: \n",
    "        raise ValueError(f\"必要列がありません: {miss}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[AGE_COL] = pd.to_numeric(df[AGE_COL], errors=\"coerce\")  # 参照用途\n",
    "\n",
    "    feat_cols = [c for c in var_list if c in df.columns]\n",
    "    if not feat_cols:\n",
    "        raise ValueError(\"var_list に一致する特徴量がありません。\")\n",
    "\n",
    "    train = df[df[PARTITION]==\"Train\"].copy()\n",
    "    test  = df[df[PARTITION]==\"Test\"].copy()\n",
    "    if train.empty or test.empty:\n",
    "        raise ValueError(\"Train/Test のいずれかが空です。partition_P1 を確認。\")\n",
    "\n",
    "    # 欠測補完\n",
    "    train_i, test_i, imp_params = impute_within_id_then_global(train, test, feat_cols)\n",
    "\n",
    "    # Robust scaling（Train fit→適用）\n",
    "    rs_params = robust_scale_fit(train_i, feat_cols)\n",
    "    train_s = robust_scale_apply(train_i, feat_cols, rs_params)\n",
    "    test_s  = robust_scale_apply(test_i,  feat_cols, rs_params)\n",
    "\n",
    "    # 段階チェック\n",
    "    _assert_no_nan(train_i, feat_cols, \"after_impute:Train\")\n",
    "    _assert_no_nan(test_i,  feat_cols, \"after_impute:Test\")\n",
    "    _assert_no_nan(train_s, feat_cols, \"after_scale:Train\")\n",
    "    _assert_no_nan(test_s,  feat_cols, \"after_scale:Test\")\n",
    "\n",
    "    # 返すメタ（残差関連キーは持たない）\n",
    "    params = {\n",
    "        \"feature_cols\": feat_cols,\n",
    "        \"imputer\": imp_params,\n",
    "        \"robust_scaler\": rs_params,\n",
    "        \"partition_col\": PARTITION,\n",
    "    }\n",
    "    # （preprocess_part1_with_varlist の末尾、return直前あたりの出力列制限パート）\n",
    "    base_meta = [ID_COL, PARTITION, AGE_COL, SEX_COL, PART_COL]\n",
    "    # ここを “あれば残す” 方式で site / diagnosis を追加\n",
    "    for opt in [SITE_COL, DIAG_COL]:\n",
    "        if opt in train_s.columns:\n",
    "            base_meta.append(opt)\n",
    "\n",
    "    cols_keep = base_meta + feat_cols\n",
    "    train_s = train_s.loc[:, [c for c in cols_keep if c in train_s.columns]].copy()\n",
    "    test_s  = test_s.loc[:,  [c for c in cols_keep if c in test_s.columns]].copy()\n",
    "    return train_s, test_s, params\n",
    "\n",
    "# ========= 軽量セーフティチェック（残差チェックはしない） =========\n",
    "def validate_pipeline_outputs(df, var_list, train_processed, test_processed, pp_params,\n",
    "                              *, eps_const=1e-12, verbose=True):\n",
    "    def _assert(cond, msg):\n",
    "        if not cond: \n",
    "            raise AssertionError(msg)\n",
    "\n",
    "    # 0) 前提\n",
    "    _assert(PARTITION in df.columns, f\"{PARTITION} が df にありません\")\n",
    "    _assert(\"robust_scaler\" in pp_params, \"pp_params['robust_scaler'] がありません\")\n",
    "\n",
    "    feat = [c for c in var_list if c in train_processed.columns]\n",
    "    _assert(len(feat) > 0, \"var_list に一致する特徴量がありません（train_processedに列が無い）\")\n",
    "\n",
    "    # 1) IDリークなし\n",
    "    ids_tr = set(df.loc[df[PARTITION]==\"Train\", ID_COL])\n",
    "    ids_te = set(df.loc[df[PARTITION]==\"Test\",  ID_COL])\n",
    "    _assert(len(ids_tr & ids_te)==0, \"Train/Test の ID が重複（リーク）\")\n",
    "\n",
    "    # 2) NaN/Inf 残りなし\n",
    "    def _nan_inf(name, data, feat_cols):\n",
    "        n_nan = int(data[feat_cols].isna().sum().sum())\n",
    "        n_inf = int(np.isinf(data[feat_cols].to_numpy()).sum())\n",
    "        if verbose:\n",
    "            print(f\"[{name}] NaN={n_nan}, Inf={n_inf}\")\n",
    "        _assert(n_nan==0, f\"{name} に NaN が残っています\")\n",
    "        _assert(n_inf==0, f\"{name} に Inf が含まれます\")\n",
    "    _nan_inf(\"Train\", train_processed, feat)\n",
    "    _nan_inf(\"Test\",  test_processed,  feat)\n",
    "\n",
    "    # 3) Robust scaler 健全性\n",
    "    sc = pp_params[\"robust_scaler\"]\n",
    "    _assert(set(feat).issubset(sc[\"median\"].index), \"robust median に特徴列が含まれていません\")\n",
    "    _assert(set(feat).issubset(sc[\"scale\"].index),  \"robust scale に特徴列が含まれていません\")\n",
    "    _assert(np.isfinite(sc[\"median\"][feat].to_numpy()).all(), \"robust median に非有限\")\n",
    "    _assert(np.isfinite(sc[\"scale\"][feat].to_numpy()).all(),  \"robust scale に非有限\")\n",
    "    _assert((sc[\"scale\"][feat] != 0).all(), \"robust scale に 0（分散0特徴）\")\n",
    "\n",
    "    # 4) “本来ありえない”特徴（Train原データ基準）\n",
    "    train_raw = df[df[PARTITION]==\"Train\"].copy()\n",
    "    raw_feats = [c for c in var_list if c in train_raw.columns]\n",
    "    raw_vals = train_raw[raw_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    all_nan_cols  = raw_vals.isna().all(axis=0)\n",
    "    fill0 = raw_vals.fillna(0.0)\n",
    "    all_zero_cols = (fill0.abs().le(1e-15)).all(axis=0)\n",
    "    def _is_const(col):\n",
    "        v = col.dropna().values\n",
    "        return True if v.size<2 else (np.nanvar(v, ddof=1) <= eps_const)\n",
    "    const_cols = raw_vals.apply(_is_const, axis=0)\n",
    "\n",
    "    offenders = {\n",
    "        \"all_nan\":  all_nan_cols[all_nan_cols].index.tolist(),\n",
    "        \"all_zero\": all_zero_cols[~all_nan_cols][all_zero_cols[~all_nan_cols]].index.tolist(),\n",
    "        \"constant\": const_cols[~all_nan_cols & ~all_zero_cols][const_cols[~all_nan_cols & ~all_zero_cols]].index.tolist(),\n",
    "    }\n",
    "    _assert(not any(offenders.values()),\n",
    "            \"“全欠損/全ゼロ/定数”特徴が見つかりました。手動で確認してください。\"\n",
    "            f\" 詳細: {{k: offenders[k][:5] for k in offenders}}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[参考] covariate分布差（前処理後の列そのまま）\")\n",
    "        def pr(s): return s.astype(str).value_counts(normalize=True).sort_index()\n",
    "        for title, s in [(\"Sex Train\", train_processed[SEX_COL]),\n",
    "                         (\"Sex Test \", test_processed[SEX_COL])]:\n",
    "            print(f\"- {title}:\")\n",
    "            try: print(pr(s).to_string())\n",
    "            except Exception: print(\"(集計不可)\")\n",
    "        if SITE_COL in train_processed.columns and SITE_COL in test_processed.columns:\n",
    "            for title, s in [(\"Site Train\", train_processed[SITE_COL]),\n",
    "                             (\"Site Test \", test_processed[SITE_COL])]:\n",
    "                print(f\"- {title}:\")\n",
    "                try: print(pr(s).to_string())\n",
    "                except Exception: print(\"(集計不可)\")\n",
    "        age_tr = pd.to_numeric(train_processed[AGE_COL], errors=\"coerce\")\n",
    "        age_te = pd.to_numeric(test_processed[AGE_COL], errors=\"coerce\")\n",
    "        print(f\"- Age mean±sd (Train)={age_tr.mean():.2f}±{age_tr.std(ddof=1):.2f}, \"\n",
    "              f\"(Test)={age_te.mean():.2f}±{age_te.std(ddof=1):.2f}\")\n",
    "\n",
    "    if verbose: \n",
    "        print(\"==== Safety Check: OK ====\")\n",
    "\n",
    "    return {\n",
    "        \"offenders\": offenders,\n",
    "    }\n",
    "\n",
    "# ========= ドライバ（前処理→軽量セーフティチェック） =========\n",
    "def run_part1_preprocess_and_validate(df, var_list, verbose=True):\n",
    "    \"\"\"\n",
    "    戻り値: train_processed, test_processed, pp_params, report\n",
    "    \"\"\"\n",
    "    # var_list列は数値化（文字は NaN → 補完で対応）\n",
    "    df = df.copy()\n",
    "    for c in var_list:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    train_processed, test_processed, pp_params = preprocess_part1_with_varlist(df, var_list)\n",
    "    report = validate_pipeline_outputs(df, var_list, train_processed, test_processed, pp_params, verbose=verbose)\n",
    "    return train_processed, test_processed, pp_params, report\n",
    "\n",
    "# ===== 実行例 =====\n",
    "train_processed, test_processed, pp_params, report = run_part1_preprocess_and_validate(df, var_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e7000f-3ca5-433d-b031-04abf3560712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from statsmodels) (2.2.6)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from statsmodels) (1.16.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ffec88-1b25-40fa-940e-4f3a9c486347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c0f73a-246f-4113-8075-a65156832855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_X_train_subject.csv  (164行×76列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_meta_train.csv  (164行×3列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_X_test_subject.csv  (41行×76列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_meta_test.csv  (41行×3列)\n",
      "[SAVE] efa_handoff_meta.json\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# ハンドオフ用ユーティリティ（EFA専用／siteは交絡に入れない）\n",
    "# ==============================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "# --------- 既定のカラム名 ---------\n",
    "ID_COL    = \"ID\"\n",
    "DIAG_COL  = \"diagnosis\"   # SCZ/HC など\n",
    "SITE_COL  = \"site\"        # 参照用途のみ（モデルには入れない）\n",
    "PART_COL  = \"Part\"        # 参照用途のみ\n",
    "AGE_COL   = \"Age\"\n",
    "SEX_COL   = \"Gender\"\n",
    "\n",
    "# --------- ユーティリティ：保存・検証 ---------\n",
    "def _ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: str, index=True):\n",
    "    try:\n",
    "        df.to_csv(path, index=index, encoding=\"utf-8\")\n",
    "        print(f\"[SAVE] {path}  ({len(df)}行×{len(df.columns)}列)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CSV保存に失敗: {path}\\n{e}\")\n",
    "\n",
    "def _validate_input(df: pd.DataFrame, cols: list, ctx: str):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"[{ctx}] 必要列がありません: {miss}\")\n",
    "\n",
    "# --------- 1) 因子分析（EFA）用の ID 平均とメタを作成＆保存 ---------\n",
    "def make_and_save_efa_inputs(train_processed: pd.DataFrame,\n",
    "                             test_processed: pd.DataFrame,\n",
    "                             feature_cols: list,\n",
    "                             output_dir: str):\n",
    "    \"\"\"\n",
    "    - EFA は ID 平均で行う想定（siteは参照保存のみ）\n",
    "    - 診断・site の最頻、訪問数をメタとして保存\n",
    "    - 出力:\n",
    "        - efa_X_train_subject.csv / efa_X_test_subject.csv （ID×features の行列）\n",
    "        - efa_meta_train.csv / efa_meta_test.csv           （診断最頻, site最頻, n_visits）\n",
    "    \"\"\"\n",
    "    _ensure_dir(output_dir)\n",
    "    _validate_input(train_processed, [ID_COL, DIAG_COL], \"EFA:train\")\n",
    "    _validate_input(test_processed,  [ID_COL, DIAG_COL], \"EFA:test\")\n",
    "\n",
    "    # --- 実在する特徴だけに揃える（var_listを基準に） ---\n",
    "    feat = [c for c in var_list if c in train_processed.columns]\n",
    "    if len(feat) == 0:\n",
    "        raise ValueError(\"EFA に使う特徴列が見つかりません。var_list を確認してください。\")\n",
    "\n",
    "    # --- ID平均（特徴） ---\n",
    "    X_tr = train_processed.groupby(ID_COL)[feat].mean()\n",
    "    X_te = test_processed.groupby(ID_COL)[feat].mean()\n",
    "\n",
    "    # --- IDメタ（診断/サイトの最頻、訪問回数） ---\n",
    "    def _meta(df):\n",
    "        g = df.groupby(ID_COL)\n",
    "        meta = pd.DataFrame({\n",
    "            DIAG_COL: g[DIAG_COL].agg(lambda s: s.mode().iloc[0] if len(s.mode())>0 else s.iloc[0]),\n",
    "            **({SITE_COL: g[SITE_COL].agg(lambda s: s.mode().iloc[0] if len(s.mode())>0 else s.iloc[0])}\n",
    "               if SITE_COL in df.columns else {}),\n",
    "            \"n_visits\": g.size()\n",
    "        })\n",
    "        return meta\n",
    "\n",
    "    meta_tr = _meta(train_processed)\n",
    "    meta_te = _meta(test_processed)\n",
    "\n",
    "    # 保存\n",
    "    _safe_to_csv(X_tr,  os.path.join(output_dir, \"efa_X_train_subject.csv\"))\n",
    "    _safe_to_csv(meta_tr,os.path.join(output_dir, \"efa_meta_train.csv\"))\n",
    "    _safe_to_csv(X_te,  os.path.join(output_dir, \"efa_X_test_subject.csv\"))\n",
    "    _safe_to_csv(meta_te,os.path.join(output_dir, \"efa_meta_test.csv\"))\n",
    "\n",
    "    return X_tr, meta_tr, X_te, meta_te\n",
    "\n",
    "# --------- 2) まとめ実行（EFA入力の作成のみ） ---------\n",
    "def handoff_efa_only(train_processed: pd.DataFrame,\n",
    "                     test_processed: pd.DataFrame,\n",
    "                     pp_params: dict,\n",
    "                     output_dir: str,\n",
    "                     feature_cols_override: list | None = None):\n",
    "    \"\"\"\n",
    "    1) EFA 入力（ID平均＋メタ）だけを保存して返す\n",
    "    - feature_cols は pp_params['feature_cols'] を既定として使用\n",
    "    - 必要なら feature_cols_override で上書き可能\n",
    "    \"\"\"\n",
    "    if feature_cols_override is not None:\n",
    "        feature_cols = list(feature_cols_override)\n",
    "    else:\n",
    "        if not isinstance(pp_params, dict) or \"feature_cols\" not in pp_params:\n",
    "            raise ValueError(\"pp_params['feature_cols'] が見つかりません。前処理の返り値を渡してください。\")\n",
    "        feature_cols = [c for c in pp_params[\"feature_cols\"] if c in train_processed.columns]\n",
    "        if len(feature_cols) == 0:\n",
    "            raise ValueError(\"EFA に使う特徴列が見つかりません。pp_params['feature_cols'] を確認してください。\")\n",
    "\n",
    "    X_tr, meta_tr, X_te, meta_te = make_and_save_efa_inputs(\n",
    "        train_processed, test_processed, feature_cols, output_dir\n",
    "    )\n",
    "\n",
    "    # 実行メタを保存（任意）\n",
    "    meta = {\n",
    "        \"n_features\": int(len(feature_cols)),\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"note\": \"GEE は実行せず、EFA 入力のみを出力しました（siteは交絡に入れず、参照用メタにのみ保存）。\"\n",
    "    }\n",
    "    try:\n",
    "        with open(os.path.join(output_dir, \"efa_handoff_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "        print(\"[SAVE] efa_handoff_meta.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] メタ保存に失敗: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_tr, \"meta_train\": meta_tr,\n",
    "        \"X_test\":  X_te, \"meta_test\":  meta_te,\n",
    "        \"run_meta\": meta\n",
    "    }\n",
    "\n",
    "# ===== 使い方例 =====\n",
    "OUTPUT_DIR = \"/Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/\"\n",
    "res_efa = handoff_efa_only(train_processed, test_processed, pp_params, output_dir=OUTPUT_DIR)\n",
    "# → 生成された CSV を、そのままあなたの run_factor_analysis_from_folder(...) に渡せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b82bb72-02d7-4233-a151-693b39919d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 因子分析に用いる特徴列数: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] horns の参照固有値を使用します。\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_parallel_scree.png\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_parallel_scree.png\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_parallel_analysis.csv (76行×2列)\n",
      "[DONE] Hornの平行分析: 推奨因子数 n_factors = 9\n",
      "[INFO] 因子分析に用いる特徴列数: 76\n",
      "[INFO] メモリ上のデータからの実行結果: n_factors = 9 （p=76, n=164）\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, os, datetime, traceback\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# ============ 設定（メタ列の既定名） ============\n",
    "ID_COL    = \"ID\"\n",
    "AGE_COL   = \"Age\"\n",
    "SEX_COL   = \"Gender\"\n",
    "SITE_COL  = \"site\"\n",
    "DIAG_COL  = \"diagnosis\"\n",
    "PART_COL  = \"Part\"\n",
    "\n",
    "META_DEFAULT_EXCLUDE = [ID_COL, AGE_COL, SEX_COL, SITE_COL, DIAG_COL, PART_COL]\n",
    "\n",
    "# ==== 先頭付近で追記（必須：ヘッドレスでも保存できるように） ====\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # 画面がなくてもPNG保存可\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==== horns の結果から参照固有値を取り出すユーティリティ ====\n",
    "\n",
    "# ==== horns の版差を吸収して呼び出すラッパ ====\n",
    "def _call_horns_parallel(Z: np.ndarray,\n",
    "                         n_iter: int,\n",
    "                         percentile: float,\n",
    "                         random_state: Optional[int]) -> object:\n",
    "    \"\"\"\n",
    "    horns.parallel_analysis の引数名ゆらぎ（n_iter/iterations/seed/percentile/quantile等）に対応。\n",
    "    戻り値（dict/obj/array）はそのまま返すので、後段は _extract_ref_eigs_from_horns(...) を使う。\n",
    "    \"\"\"\n",
    "    import inspect, horns\n",
    "    fn = getattr(horns, \"parallel_analysis\", None)\n",
    "    if fn is None:\n",
    "        # 実装によっては名称が違うことがある\n",
    "        fn = getattr(horns, \"parallel\", None)\n",
    "    if fn is None:\n",
    "        raise RuntimeError(\"horns に parallel_analysis (または parallel) が見つかりません。\")\n",
    "\n",
    "    sig = inspect.signature(fn)\n",
    "    params = set(sig.parameters.keys())\n",
    "\n",
    "    kwargs = {}\n",
    "\n",
    "    # 反復回数\n",
    "    for key in [\"n_iter\", \"iterations\", \"n_iterations\", \"replications\", \"B\", \"ndraw\", \"n\"]:\n",
    "        if key in params:\n",
    "            kwargs[key] = n_iter\n",
    "            break\n",
    "\n",
    "    # 乱数種\n",
    "    for key in [\"random_state\", \"seed\", \"rng\"]:\n",
    "        if key in params:\n",
    "            kwargs[key] = random_state\n",
    "            break\n",
    "\n",
    "    # パーセンタイル/分位\n",
    "    if \"percentile\" in params:\n",
    "        # 多くは 0-100 スケール\n",
    "        kwargs[\"percentile\"] = percentile\n",
    "    elif \"quantile\" in params:\n",
    "        # 0-1 を期待する実装向け\n",
    "        kwargs[\"quantile\"] = (percentile/100.0 if percentile > 1 else percentile)\n",
    "    elif \"q\" in params:\n",
    "        kwargs[\"q\"] = (percentile/100.0 if percentile > 1 else percentile)\n",
    "\n",
    "    # 呼び出し（Z は位置引数で渡すのが最も互換的）\n",
    "    return fn(Z, **kwargs)\n",
    "\n",
    "\n",
    "def _extract_ref_eigs_from_horns(res, percentile: float, p: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    horns.parallel_analysis(...) の返り値から、参照固有値ベクトル（長さ p）を取り出す。\n",
    "    返り値が dict / オブジェクト / そのまま配列のどれでも動くように防御的に実装。\n",
    "    \"\"\"\n",
    "    # 1) dict の可能性\n",
    "    if isinstance(res, dict):\n",
    "        for k in [\"percentile_eig\", \"percentile_eigenvalues\", \"ref_eigs\"]:\n",
    "            if k in res:\n",
    "                arr = np.asarray(res[k], dtype=float).ravel()\n",
    "                break\n",
    "        else:\n",
    "            # dict だが上記キーが無い → とりあえず最初の数値配列らしき値を探す\n",
    "            arr = None\n",
    "            for v in res.values():\n",
    "                try:\n",
    "                    cand = np.asarray(v, dtype=float).ravel()\n",
    "                    if cand.size >= p:\n",
    "                        arr = cand\n",
    "                        break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if arr is None:\n",
    "                raise RuntimeError(\"horns結果から参照固有値を抽出できませんでした（dict）。\")\n",
    "    else:\n",
    "        # 2) オブジェクト属性の可能性\n",
    "        for k in [\"percentile_eig\", \"percentile_eigenvalues\", \"ref_eigs\"]:\n",
    "            if hasattr(res, k):\n",
    "                arr = np.asarray(getattr(res, k), dtype=float).ravel()\n",
    "                break\n",
    "        else:\n",
    "            # 3) res 自体が配列\n",
    "            try:\n",
    "                arr = np.asarray(res, dtype=float).ravel()\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"horns結果から参照固有値を抽出できませんでした。\")\n",
    "\n",
    "    # 長さ調整（念のため）\n",
    "    if arr.size < p:\n",
    "        # 不足分は末尾を繰り返して埋める（ほぼ起きないはず）\n",
    "        arr = np.pad(arr, (0, p - arr.size), mode=\"edge\")\n",
    "    elif arr.size > p:\n",
    "        arr = arr[:p]\n",
    "\n",
    "    # 多くの実装では昇順/未ソートなので、観測固有値との比較に合わせて降順へ\n",
    "    arr = np.sort(arr)[::-1]\n",
    "    return arr\n",
    "\n",
    "# ==== 観測固有値（降順） ====\n",
    "def _observed_eigs_from_corr(Z: np.ndarray) -> np.ndarray:\n",
    "    R = np.corrcoef(Z, rowvar=False)\n",
    "    R = np.nan_to_num(R, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return np.linalg.eigvalsh(R)[::-1]\n",
    "\n",
    "# ==== 平行分析図＋スクリープロット ====\n",
    "def plot_parallel_and_scree(\n",
    "    obs_eigs: np.ndarray,\n",
    "    ref_eigs: np.ndarray | None,\n",
    "    n_factors: int | None,\n",
    "    title: str,\n",
    "    out_path: str,\n",
    "    dpi: int = 150,\n",
    ") -> None:\n",
    "    x = np.arange(1, len(obs_eigs) + 1)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(x, obs_eigs, marker=\"o\", linewidth=1.5, label=\"Observed eigenvalues\")\n",
    "    if ref_eigs is not None:\n",
    "        m = min(len(obs_eigs), len(ref_eigs))\n",
    "        plt.plot(x[:m], ref_eigs[:m], marker=\"s\", linestyle=\"--\", linewidth=1.2,\n",
    "                 label=\"Reference (95th percentile)\")\n",
    "    if n_factors is not None and 1 <= n_factors <= len(obs_eigs):\n",
    "        plt.axvline(n_factors, color=\"gray\", linestyle=\":\", linewidth=1.5)\n",
    "        plt.text(n_factors + 0.3, max(obs_eigs)*0.95, f\"k = {n_factors}\",\n",
    "                 fontsize=10, va=\"top\", ha=\"left\")\n",
    "    plt.xlabel(\"Component / Factor rank\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=dpi)\n",
    "    plt.close()\n",
    "    print(f\"[SAVE] {out_path}\")\n",
    "\n",
    "# ==== horns を使った平行分析 → 図を保存する便利関数 ====\n",
    "def run_parallel_with_horns_and_plot(\n",
    "    X_feat: pd.DataFrame,\n",
    "    n_iter: int = 600,\n",
    "    percentile: float = 95.0,\n",
    "    random_state: int = 42,\n",
    "    out_png: str = \"efa_parallel_scree.png\",\n",
    "    title: str | None = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray | None, int]:\n",
    "    \"\"\"\n",
    "    - X_feat: 特徴行列（行=サブジェクト, 列=特徴）※ z標準化は内部で実施\n",
    "    - horns の参照固有値を使い、図を保存して (obs, ref, n_factors) を返す\n",
    "    \"\"\"\n",
    "    # 列標準化\n",
    "    Z = (X_feat - X_feat.mean()) / X_feat.std(ddof=1).replace(0, np.nan)\n",
    "    Z = Z.fillna(0.0).to_numpy(dtype=float)\n",
    "    n, p = Z.shape\n",
    "\n",
    "    # 観測固有値（降順）\n",
    "    obs = _observed_eigs_from_corr(Z)\n",
    "\n",
    "\n",
    "    # horns 実行（失敗時は None）\n",
    "    ref = None\n",
    "    try:\n",
    "        import horns\n",
    "        res = _call_horns_parallel(\n",
    "            Z, n_iter=n_iter, percentile=percentile, random_state=random_state\n",
    "        )\n",
    "        ref = _extract_ref_eigs_from_horns(res, percentile=percentile, p=p)\n",
    "        print(\"[INFO] horns の参照固有値を使用します。\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] horns 参照値の取得に失敗: {e}\\n→ スクリープロットのみ描画します。\")\n",
    "        ref = None\n",
    "        \n",
    "    # 因子数（ホーン基準）：obs > ref の個数\n",
    "    if ref is not None:\n",
    "        n_factors = int(np.sum(obs[:len(ref)] > ref[:len(obs)]))\n",
    "        if n_factors < 1:\n",
    "            n_factors = 1\n",
    "    else:\n",
    "        n_factors = None\n",
    "\n",
    "    # 図保存\n",
    "    if title is None:\n",
    "        title = f\"Parallel Analysis (p={p}, n={n})\"\n",
    "    plot_parallel_and_scree(\n",
    "        obs_eigs=obs, ref_eigs=ref, n_factors=n_factors,\n",
    "        title=title, out_path=out_png, dpi=150\n",
    "    )\n",
    "    return obs, ref, (n_factors if n_factors is not None else -1)\n",
    "\n",
    "\n",
    "# ============ ユーティリティ：保存系 ============\n",
    "def _ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _safe_csv(df: pd.DataFrame, path: str):\n",
    "    try:\n",
    "        df.to_csv(path, encoding=\"utf-8\", index=True)\n",
    "        print(f\"[SAVE] {path} ({df.shape[0]}行×{df.shape[1]}列)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CSV保存失敗: {path}\\n{e}\")\n",
    "\n",
    "# ============ 入力整備：メタ除外＆数値列のみ抽出 ============\n",
    "def _select_feature_matrix(X: pd.DataFrame,\n",
    "                           exclude_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - exclude_cols（例：['ID','diagnosis','Age','Gender','site','Part']）を除外\n",
    "    - 数値列のみ残す（非数値は除外）\n",
    "    - 全ゼロ分散列は安全のため除外（相関が定義できないため）\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X).copy()\n",
    "    if exclude_cols:\n",
    "        drop_cols = [c for c in exclude_cols if c in X.columns]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols)\n",
    "            print(f\"[INFO] メタ列を除外: {drop_cols}\")\n",
    "\n",
    "    # 数値化（非数値は落とす）\n",
    "    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    X = X[num_cols].copy()\n",
    "    if len(X.columns) == 0:\n",
    "        raise ValueError(\"数値の特徴列がありません。入力にメタ列しかない可能性があります。\")\n",
    "\n",
    "    # ゼロ分散列の除外\n",
    "    std = X.std(ddof=1)\n",
    "    zero_vars = std.index[std == 0].tolist()\n",
    "    if zero_vars:\n",
    "        X = X.drop(columns=zero_vars)\n",
    "        print(f\"[INFO] ゼロ分散列を除外: {zero_vars}\")\n",
    "\n",
    "    print(f\"[INFO] 因子分析に用いる特徴列数: {X.shape[1]}\")\n",
    "    return X\n",
    "\n",
    "# ============ 平行分析（Horn's Parallel Analysis） ============\n",
    "def parallel_analysis(\n",
    "    X: pd.DataFrame,\n",
    "    n_iter: int = 500,\n",
    "    percentile: float = 95.0,\n",
    "    random_state: Optional[int] = 0,\n",
    "    use_horns_pkg: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Horn の平行分析で因子数を決定（site 等のメタは考慮しない）。\n",
    "    - X: n個体×p変数（特徴のみ）\n",
    "    - 戻り値: (obs_eigs, ref_eigs, n_factors)\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X).copy()\n",
    "\n",
    "    # 相関行列ベース（列標準化）\n",
    "    Z = (X - X.mean()) / X.std(ddof=1).replace(0, np.nan)\n",
    "    Z = Z.fillna(0.0).to_numpy(dtype=float)\n",
    "\n",
    "    n, p = Z.shape\n",
    "    if p < 1 or n < 2:\n",
    "        raise ValueError(\"平行分析の入力Xの形状が不正です。\")\n",
    "\n",
    "    # 観測固有値\n",
    "    R = np.corrcoef(Z, rowvar=False)\n",
    "    R = np.nan_to_num(R, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    obs_eigs = np.linalg.eigvalsh(R)[::-1]\n",
    "\n",
    "    # 参照固有値（horns があれば利用）\n",
    "    ref_eigs: Optional[np.ndarray] = None\n",
    "    if use_horns_pkg:\n",
    "        try:\n",
    "            import horns\n",
    "            res = horns.parallel_analysis(Z, n_iter=n_iter, percentile=percentile, random_state=random_state)\n",
    "\n",
    "            # 返り値の形に頑健に対応（dict/オブジェクト/配列）\n",
    "            cand = None\n",
    "            if isinstance(res, dict):\n",
    "                for k in [\"percentile_eig\", \"percentile_eigenvalues\", \"ref_eigs\"]:\n",
    "                    if k in res:\n",
    "                        cand = res[k]; break\n",
    "            if cand is None:\n",
    "                for k in [\"percentile_eig\", \"percentile_eigenvalues\", \"ref_eigs\"]:\n",
    "                    if hasattr(res, k):\n",
    "                        cand = getattr(res, k); break\n",
    "            if cand is None:\n",
    "                # res 自体が配列っぽい場合\n",
    "                try:\n",
    "                    cand = np.asarray(res)\n",
    "                except Exception:\n",
    "                    cand = None\n",
    "\n",
    "            if cand is None:\n",
    "                raise RuntimeError(\"horns.parallel_analysis の返り値から参照固有値を取得できません。\")\n",
    "\n",
    "            ref = np.asarray(cand, dtype=float).ravel()\n",
    "            ref_eigs = ref[::-1]  # ← ここが正しいスライス\n",
    "        except Exception:\n",
    "            # horns が使えない/取得失敗 → 自前実装にフォールバック\n",
    "            ref_eigs = None\n",
    "\n",
    "    # 自前実装（N(0,1) の無相関データから分位点を推定）\n",
    "    if ref_eigs is None:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        sim_eigs = np.zeros((n_iter, p), dtype=float)\n",
    "        for b in range(n_iter):\n",
    "            Xb = rng.standard_normal(size=(n, p))\n",
    "            Rb = np.corrcoef(Xb, rowvar=False)\n",
    "            lam = np.linalg.eigvalsh(Rb)[::-1]\n",
    "            sim_eigs[b, :] = lam\n",
    "        ref_eigs = np.percentile(sim_eigs, percentile, axis=0)\n",
    "\n",
    "    # 因子数決定\n",
    "    n_factors = int(np.sum(obs_eigs > ref_eigs))\n",
    "    if n_factors < 1:\n",
    "        n_factors = 1\n",
    "\n",
    "    return obs_eigs, ref_eigs, n_factors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "def plot_parallel_and_scree(\n",
    "    obs_eigs: Iterable[float],\n",
    "    ref_eigs: Optional[Iterable[float]] = None,\n",
    "    n_factors: Optional[int] = None,\n",
    "    title: str = \"Parallel Analysis / Scree\",\n",
    "    out_path: str = \"efa_parallel_scree.png\",\n",
    "    dpi: int = 150\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    平行分析図（観測固有値 vs 参照固有値95%分位）とスクリープロットを重ね描画して保存する。\n",
    "    - obs_eigs: 観測固有値（降順）\n",
    "    - ref_eigs: 参照固有値（降順; None可）\n",
    "    - n_factors: 推奨因子数（縦線を引く; None可）\n",
    "    \"\"\"\n",
    "    obs = np.asarray(list(obs_eigs), dtype=float)\n",
    "    x = np.arange(1, len(obs) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # 観測固有値（折れ線）\n",
    "    plt.plot(x, obs, marker=\"o\", linewidth=1.5, label=\"Observed eigenvalues\")\n",
    "\n",
    "    # 参照固有値（あれば）\n",
    "    if ref_eigs is not None:\n",
    "        ref = np.asarray(list(ref_eigs), dtype=float)\n",
    "        # 長さが違う場合は短い方に合わせる\n",
    "        m = min(len(obs), len(ref))\n",
    "        plt.plot(x[:m], ref[:m], marker=\"s\", linestyle=\"--\", linewidth=1.2,\n",
    "                 label=\"Reference (95th percentile)\")\n",
    "\n",
    "    # 推奨因子数のガイド線（あれば）\n",
    "    if n_factors is not None and 1 <= n_factors <= len(obs):\n",
    "        plt.axvline(n_factors, color=\"gray\", linestyle=\":\", linewidth=1.5)\n",
    "        plt.text(n_factors + 0.3, max(obs)*0.95, f\"k = {n_factors}\",\n",
    "                 fontsize=10, va=\"top\", ha=\"left\")\n",
    "\n",
    "    plt.xlabel(\"Component / Factor rank\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=dpi)\n",
    "    plt.close()\n",
    "    print(f\"[SAVE] {out_path}\")\n",
    "\n",
    "# ==============================\n",
    "# 平行分析（Horn） 実行パート\n",
    "# ==============================\n",
    "import os, json, traceback\n",
    "\n",
    "# ---- パラメータ（必要に応じ変更）----\n",
    "OUTPUT_DIR   = \"/Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing\"\n",
    "N_ITER       = 600          # シミュレーション反復数\n",
    "PERCENTILE   = 95.0         # 参照固有値の分位点（Horn 既定）\n",
    "RANDOM_STATE = 42\n",
    "USE_HORNS    = True         # horns が無ければ自動で自作法にフォールバック\n",
    "\n",
    "# ---- 1) フォルダの CSV から実行（おすすめ：前処理の出力を使う）----\n",
    "try:\n",
    "    x_path = os.path.join(OUTPUT_DIR, \"efa_X_train_subject.csv\")\n",
    "    if not os.path.exists(x_path):\n",
    "        raise FileNotFoundError(f\"{x_path} が見つかりません。前処理で EFA 入力を保存してください。\")\n",
    "\n",
    "    X_in   = pd.read_csv(x_path, index_col=0)\n",
    "    # 念のため：メタ列が混ざっていても除外＆数値列だけにする\n",
    "    X_feat = _select_feature_matrix(X_in, exclude_cols=META_DEFAULT_EXCLUDE)\n",
    "    \n",
    "    # 例：X_feat は _select_feature_matrix(...) 済みの特徴行列\n",
    "    png_path = os.path.join(OUTPUT_DIR, \"efa_parallel_scree.png\")\n",
    "    obs_eigs, ref_eigs, n_factors = run_parallel_with_horns_and_plot(\n",
    "        X_feat,\n",
    "        n_iter=N_ITER, percentile=PERCENTILE, random_state=RANDOM_STATE,\n",
    "        out_png=png_path,\n",
    "        title=f\"Parallel Analysis (p={X_feat.shape[1]}, n={X_feat.shape[0]})\"\n",
    "    )\n",
    "\n",
    "\n",
    "    obs_eigs, ref_eigs, n_factors = parallel_analysis(\n",
    "        X_feat, n_iter=N_ITER, percentile=PERCENTILE,\n",
    "        random_state=RANDOM_STATE, use_horns_pkg=USE_HORNS\n",
    "    )\n",
    "    # 図を保存（平行分析＋スクリープロット）\n",
    "    plot_parallel_and_scree(\n",
    "        obs_eigs=obs_eigs,\n",
    "        ref_eigs=ref_eigs,                 # NoneでもOK\n",
    "        n_factors=n_factors,               # 縦線ガイド\n",
    "        title=f\"Parallel Analysis (p={X_feat.shape[1]}, n={X_feat.shape[0]})\",\n",
    "        out_path=os.path.join(OUTPUT_DIR, \"efa_parallel_scree.png\"),\n",
    "        dpi=150\n",
    "    )\n",
    "\n",
    "    # 保存\n",
    "    df_eigs = pd.DataFrame({\n",
    "        \"eigen_observed\": obs_eigs,\n",
    "        f\"eigen_ref_p{int(PERCENTILE)}\": ref_eigs\n",
    "    })\n",
    "    df_eigs.index.name = \"rank\"\n",
    "    _safe_csv(df_eigs, os.path.join(OUTPUT_DIR, \"efa_parallel_analysis.csv\"))\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, \"efa_parallel_analysis_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"n_factors\": int(n_factors),\n",
    "            \"n_variables\": int(X_feat.shape[1]),\n",
    "            \"n_samples\": int(X_feat.shape[0]),\n",
    "            \"n_iter\": int(N_ITER),\n",
    "            \"percentile\": float(PERCENTILE),\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"used_horns_pkg\": bool(USE_HORNS)\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[DONE] Hornの平行分析: 推奨因子数 n_factors = {n_factors}\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] フォルダからの平行分析に失敗:\", e)\n",
    "    print(traceback.format_exc(limit=1))\n",
    "\n",
    "# ---- 2) （代替）メモリ上のデータから直接実行する場合 ----\n",
    "# train_processed / pp_params がメモリにあるときのみ使用\n",
    "try:\n",
    "    if 'train_processed' in globals() and 'pp_params' in globals():\n",
    "        feat_cols = [c for c in pp_params.get(\"feature_cols\", []) if c in train_processed.columns]\n",
    "        if len(feat_cols) == 0:\n",
    "            raise ValueError(\"pp_params['feature_cols'] が空 or 列が見つかりません。\")\n",
    "\n",
    "        # ID 平均（EFA は ID×特徴 の行列を想定）※site 等のメタは使いません\n",
    "        X_tr_subj = train_processed.groupby(ID_COL)[feat_cols].mean()\n",
    "        X_feat2   = _select_feature_matrix(X_tr_subj, exclude_cols=META_DEFAULT_EXCLUDE)\n",
    "\n",
    "        obs2, ref2, n2 = parallel_analysis(\n",
    "            X_feat2, n_iter=N_ITER, percentile=PERCENTILE,\n",
    "            random_state=RANDOM_STATE, use_horns_pkg=USE_HORNS\n",
    "        )\n",
    "        print(f\"[INFO] メモリ上のデータからの実行結果: n_factors = {n2} （p={X_feat2.shape[1]}, n={X_feat2.shape[0]}）\")\n",
    "    else:\n",
    "        print(\"[SKIP] train_processed / pp_params が見つからないため、メモリ実行はスキップしました。\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] メモリからの平行分析に失敗:\", e)\n",
    "    print(traceback.format_exc(limit=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf02e9e-1ee3-4ada-a901-db75bfa0db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] representatives.json / representatives.csv / representatives_selection_log.csv -> /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/\n",
      "代表9本: ['num_INTJ_ratio', 'num_ADV_ratio', 'total_tree_height_CEJCminus_ratio', 'num_shuujoshi_ratio', 'num_sentences', 'w2Vsim_mean', 'obj_ratio', 'nmod_ratio', 'num_rentaishi_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>feature</th>\n",
       "      <th>lambda_abs</th>\n",
       "      <th>max_other_abs</th>\n",
       "      <th>gap_abs</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Factor1</td>\n",
       "      <td>num_INTJ_ratio</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.207208</td>\n",
       "      <td>0.727763</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Factor2</td>\n",
       "      <td>num_ADV_ratio</td>\n",
       "      <td>0.949759</td>\n",
       "      <td>0.178751</td>\n",
       "      <td>0.771008</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Factor3</td>\n",
       "      <td>total_tree_height_CEJCminus_ratio</td>\n",
       "      <td>0.850803</td>\n",
       "      <td>0.230743</td>\n",
       "      <td>0.620060</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Factor4</td>\n",
       "      <td>num_shuujoshi_ratio</td>\n",
       "      <td>0.937765</td>\n",
       "      <td>0.174738</td>\n",
       "      <td>0.763027</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Factor5</td>\n",
       "      <td>num_sentences</td>\n",
       "      <td>0.882992</td>\n",
       "      <td>0.216394</td>\n",
       "      <td>0.666598</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Factor6</td>\n",
       "      <td>w2Vsim_mean</td>\n",
       "      <td>0.903174</td>\n",
       "      <td>0.145323</td>\n",
       "      <td>0.757850</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Factor7</td>\n",
       "      <td>obj_ratio</td>\n",
       "      <td>0.617361</td>\n",
       "      <td>0.261333</td>\n",
       "      <td>0.356028</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Factor8</td>\n",
       "      <td>nmod_ratio</td>\n",
       "      <td>0.763392</td>\n",
       "      <td>0.259735</td>\n",
       "      <td>0.503656</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Factor9</td>\n",
       "      <td>num_rentaishi_ratio</td>\n",
       "      <td>0.754928</td>\n",
       "      <td>0.208617</td>\n",
       "      <td>0.546311</td>\n",
       "      <td>strict</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    factor                            feature  lambda_abs  max_other_abs  \\\n",
       "0  Factor1                     num_INTJ_ratio    0.934971       0.207208   \n",
       "1  Factor2                      num_ADV_ratio    0.949759       0.178751   \n",
       "2  Factor3  total_tree_height_CEJCminus_ratio    0.850803       0.230743   \n",
       "3  Factor4                num_shuujoshi_ratio    0.937765       0.174738   \n",
       "4  Factor5                      num_sentences    0.882992       0.216394   \n",
       "5  Factor6                        w2Vsim_mean    0.903174       0.145323   \n",
       "6  Factor7                          obj_ratio    0.617361       0.261333   \n",
       "7  Factor8                         nmod_ratio    0.763392       0.259735   \n",
       "8  Factor9                num_rentaishi_ratio    0.754928       0.208617   \n",
       "\n",
       "    gap_abs decision  \n",
       "0  0.727763   strict  \n",
       "1  0.771008   strict  \n",
       "2  0.620060   strict  \n",
       "3  0.763027   strict  \n",
       "4  0.666598   strict  \n",
       "5  0.757850   strict  \n",
       "6  0.356028   strict  \n",
       "7  0.503656   strict  \n",
       "8  0.546311   strict  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_per_factor.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor1.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor2.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor3.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor4.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor5.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor6.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor7.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor8.csv\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/efa_top10_Factor9.csv\n",
      "     factor  rank                  feature   loading  abs_loading  \\\n",
      "0   Factor1     1           num_INTJ_ratio  0.934971     0.934971   \n",
      "1   Factor1     2          discourse_ratio  0.926837     0.926837   \n",
      "2   Factor1     3   num_intj_general_ratio  0.896400     0.896400   \n",
      "3   Factor1     4          num_SCONJ_ratio -0.829499     0.829499   \n",
      "4   Factor1     5          num_PUNCT_ratio  0.822562     0.822562   \n",
      "5   Factor1     6              advcl_ratio -0.792839     0.792839   \n",
      "6   Factor1     7  num_setuzokujoshi_ratio -0.759694     0.759694   \n",
      "7   Factor1     8           num_VERB_ratio -0.748605     0.748605   \n",
      "8   Factor1     9  perseveration_rrb_ratio  0.737410     0.737410   \n",
      "9   Factor1    10         duplicates_ratio -0.727225     0.727225   \n",
      "10  Factor2     1            num_ADV_ratio  0.949759     0.949759   \n",
      "11  Factor2     2             advmod_ratio  0.934037     0.934037   \n",
      "12  Factor2     3       senti_sim_mean_rrb -0.508705     0.508705   \n",
      "13  Factor2     4           senti_sim_mean -0.419107     0.419107   \n",
      "14  Factor2     5           num_NOUN_ratio -0.408930     0.408930   \n",
      "\n",
      "    cross_max_abs cross_max_factor   gap_abs  passes_loading  passes_gap  \\\n",
      "0        0.207208          Factor8  0.727763            True        True   \n",
      "1        0.263533          Factor8  0.663304            True        True   \n",
      "2        0.215702          Factor9  0.680698            True        True   \n",
      "3        0.240622          Factor5  0.588877            True        True   \n",
      "4        0.409111          Factor7  0.413451            True        True   \n",
      "5        0.279407          Factor9  0.513431            True        True   \n",
      "6        0.278838          Factor5  0.480855            True        True   \n",
      "7        0.275785          Factor2  0.472820            True        True   \n",
      "8        0.286937          Factor9  0.450473            True        True   \n",
      "9        0.550301          Factor5  0.176924            True       False   \n",
      "10       0.178751          Factor8  0.771008            True        True   \n",
      "11       0.152664          Factor8  0.781373            True        True   \n",
      "12       0.437831          Factor8  0.070874           False       False   \n",
      "13       0.361917          Factor1  0.057189           False       False   \n",
      "14       0.663632          Factor8 -0.254703           False       False   \n",
      "\n",
      "    passes_cross  passes_all  \n",
      "0           True        True  \n",
      "1           True        True  \n",
      "2           True        True  \n",
      "3           True        True  \n",
      "4          False       False  \n",
      "5           True        True  \n",
      "6           True        True  \n",
      "7           True        True  \n",
      "8           True        True  \n",
      "9          False       False  \n",
      "10          True        True  \n",
      "11          True        True  \n",
      "12         False       False  \n",
      "13          True       False  \n",
      "14         False       False  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def select_top_per_factor_9(\n",
    "    loadings: pd.DataFrame,\n",
    "    n_factors: int = 8,\n",
    "    min_loading: Optional[float] = 0.60,   # 例の基準 |λ|≥0.60\n",
    "    min_gap: Optional[float] = 0.15,       # 当該因子と他因子の差（Δloading）\n",
    "    cross_max: Optional[float] = 0.4,     # 他因子での最大絶対負荷（クロス負荷上限）\n",
    "    save_dir: Optional[str] = None         # 保存先（代表のJSON/CSVを出力）\n",
    ") -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    各因子の“絶対値で最大”の特徴を一つずつ選び、9本（=n_factors）を返す。\n",
    "    - 閾値はオプション。満たさなければ緩和して必ず1本は選ぶ（フォールバック）。\n",
    "    - 返り値: (representatives, details_df)\n",
    "    \"\"\"\n",
    "    # loadings: 行=特徴, 列=F1..Fm の DataFrame を想定\n",
    "    if loadings is None or loadings.empty:\n",
    "        raise ValueError(\"loadings が空です。EFAの結果を確認してください。\")\n",
    "\n",
    "    # 列（因子）を先頭から n_factors 本\n",
    "    fac_cols = list(loadings.columns)\n",
    "    if len(fac_cols) == 0:\n",
    "        raise ValueError(\"因子列が見つかりません。\")\n",
    "    if n_factors > len(fac_cols):\n",
    "        n_factors = len(fac_cols)  # ある分だけに自動調整\n",
    "    fac_cols = fac_cols[:n_factors]\n",
    "\n",
    "    # 絶対値の負荷行列\n",
    "    Labs = loadings.abs().copy()\n",
    "\n",
    "    selected: List[str] = []\n",
    "    rows = []  # 詳細ログ\n",
    "\n",
    "    for f in fac_cols:\n",
    "        ser = Labs[f].sort_values(ascending=False)  # 当該因子で絶対値の大きい順\n",
    "        chosen = None\n",
    "        reason = \"strict\"\n",
    "\n",
    "        # 1) まずは厳格基準でスキャン\n",
    "        for feat, lam in ser.items():\n",
    "            if feat in selected:\n",
    "                continue\n",
    "            others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "            max_other = float(others.max()) if len(others) else 0.0\n",
    "            gap = float(lam - max_other)\n",
    "\n",
    "            ok_loading = (min_loading is None) or (lam >= min_loading)\n",
    "            ok_gap     = (min_gap is None)     or (gap >= min_gap)\n",
    "            ok_cross   = (cross_max is None)   or (max_other <= cross_max)\n",
    "\n",
    "            if ok_loading and ok_gap and ok_cross:\n",
    "                chosen = (feat, float(lam), max_other, gap)\n",
    "                break\n",
    "\n",
    "        # 2) 緩和（基準を緩めてでも必ず1本）\n",
    "        if chosen is None:\n",
    "            # まず“既に選んだ特徴を除く”前提で最大を取る\n",
    "            for feat, lam in ser.items():\n",
    "                if feat not in selected:\n",
    "                    others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "                    max_other = float(others.max()) if len(others) else 0.0\n",
    "                    gap = float(lam - max_other)\n",
    "                    chosen = (feat, float(lam), max_other, gap)\n",
    "                    reason = \"relaxed\"\n",
    "                    break\n",
    "\n",
    "        if chosen is None:\n",
    "            raise RuntimeError(f\"因子 {f} で代表を選べませんでした。loadingsを確認してください。\")\n",
    "\n",
    "        feat, lam, max_other, gap = chosen\n",
    "        selected.append(feat)\n",
    "        rows.append({\n",
    "            \"factor\": f,\n",
    "            \"feature\": feat,\n",
    "            \"lambda_abs\": lam,\n",
    "            \"max_other_abs\": max_other,\n",
    "            \"gap_abs\": gap,\n",
    "            \"decision\": reason\n",
    "        })\n",
    "\n",
    "    details_df = pd.DataFrame(rows)\n",
    "\n",
    "    # 代表の保存（任意）\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # JSON（handoffの多変量GEEで読める形式）\n",
    "        with open(os.path.join(save_dir, \"representatives.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"representatives\": selected}, f, ensure_ascii=False, indent=2)\n",
    "        # 表形式でも保存\n",
    "        df_rep = pd.DataFrame({\"feature\": selected, \"factor\": fac_cols[:len(selected)]})\n",
    "        df_rep.to_csv(os.path.join(save_dir, \"representatives.csv\"), index=False, encoding=\"utf-8\")\n",
    "        # 選定ログ\n",
    "        details_df.to_csv(os.path.join(save_dir, \"representatives_selection_log.csv\"), index=False, encoding=\"utf-8\")\n",
    "        print(f\"[SAVE] representatives.json / representatives.csv / representatives_selection_log.csv -> {save_dir}\")\n",
    "\n",
    "    return selected, details_df\n",
    "\n",
    "# =====================================\n",
    "# 代表9本の選定（res が無いときの安全版）\n",
    "# =====================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ここはあなたの保存先に合わせて\n",
    "OUTPUT_DIR = \"/Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_efa_preprocess/\"\n",
    "\n",
    "# 1) loadings を用意：res があればそれを、無ければ CSV から読む\n",
    "if 'res' in globals() and isinstance(res, dict) and 'loadings' in res:\n",
    "    loadings_df = res['loadings']\n",
    "else:\n",
    "    loadings_path = os.path.join(OUTPUT_DIR, \"efa_loadings.csv\")\n",
    "    if not os.path.exists(loadings_path):\n",
    "        raise FileNotFoundError(f\"{loadings_path} がありません。先に因子分析を実行してefa_loadings.csvを出力してください。\")\n",
    "    loadings_df = pd.read_csv(loadings_path, index_col=0)\n",
    "\n",
    "# 2) 代表選定を実行\n",
    "reps, log = select_top_per_factor_9(\n",
    "    loadings=loadings_df,\n",
    "    n_factors=9,          # Hornの結果に合わせて必要なら調整\n",
    "    min_loading=0.60,\n",
    "    min_gap=0.15,\n",
    "    cross_max=0.4,\n",
    "    save_dir=OUTPUT_DIR   # representatives.json / .csv / ログ が保存されます\n",
    ")\n",
    "\n",
    "print(\"代表9本:\", reps)\n",
    "display(log.head(12))\n",
    "\n",
    "# === 追加: 各因子の高負荷 Top10 を出力（絶対値で順位付け、符号は保持） ===\n",
    "TOP_K = 10\n",
    "\n",
    "# しきい値（本文基準に合わせておく）\n",
    "TH_LOAD   = 0.60   # |λ| ≥ 0.60\n",
    "TH_GAP    = 0.20   # 当該因子と他因子の |λ| 差\n",
    "TH_CROSS  = 0.4   # 他因子での最大 |λ|\n",
    "\n",
    "# 因子列（最大9まで）\n",
    "n_factors_out = min(9, loadings_df.shape[1])\n",
    "fac_cols = list(loadings_df.columns[:n_factors_out])\n",
    "\n",
    "# 絶対値負荷\n",
    "Labs = loadings_df.abs().copy()\n",
    "\n",
    "rows = []\n",
    "for f in fac_cols:\n",
    "    # 当該因子の絶対値で TopK\n",
    "    ser_abs = Labs[f].sort_values(ascending=False).head(TOP_K)\n",
    "    for rank, (feat, lam_abs) in enumerate(ser_abs.items(), start=1):\n",
    "        lam_signed = float(loadings_df.loc[feat, f])\n",
    "        # 他因子の最大絶対負荷とその因子名\n",
    "        other_cols = [c for c in fac_cols if c != f]\n",
    "        if other_cols:\n",
    "            other_abs = Labs.loc[feat, other_cols]\n",
    "            cross_max_abs = float(other_abs.max())\n",
    "            cross_max_factor = str(other_abs.idxmax())\n",
    "        else:\n",
    "            cross_max_abs = 0.0\n",
    "            cross_max_factor = None\n",
    "        gap_abs = float(lam_abs - cross_max_abs)\n",
    "\n",
    "        rows.append({\n",
    "            \"factor\": f,\n",
    "            \"rank\": rank,\n",
    "            \"feature\": feat,\n",
    "            \"loading\": lam_signed,          # 符号つき\n",
    "            \"abs_loading\": lam_abs,         # 絶対値での順位基準\n",
    "            \"cross_max_abs\": cross_max_abs, # 他因子での最大 |λ|\n",
    "            \"cross_max_factor\": cross_max_factor,\n",
    "            \"gap_abs\": gap_abs,             # 当該因子と他因子の差\n",
    "            \"passes_loading\": lam_abs >= TH_LOAD,\n",
    "            \"passes_gap\": gap_abs >= TH_GAP,\n",
    "            \"passes_cross\": cross_max_abs <= TH_CROSS\n",
    "        })\n",
    "\n",
    "df_top10 = pd.DataFrame(rows)\n",
    "df_top10[\"passes_all\"] = df_top10[\"passes_loading\"] & df_top10[\"passes_gap\"] & df_top10[\"passes_cross\"]\n",
    "\n",
    "# まとめファイル\n",
    "out_all = os.path.join(OUTPUT_DIR, \"efa_top10_per_factor.csv\")\n",
    "df_top10.to_csv(out_all, index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", out_all)\n",
    "\n",
    "# 因子ごとに個別ファイルも保存（任意）\n",
    "for f in fac_cols:\n",
    "    out_f = os.path.join(OUTPUT_DIR, f\"efa_top10_{f}.csv\")\n",
    "    df_top10[df_top10[\"factor\"] == f].to_csv(out_f, index=False, encoding=\"utf-8\")\n",
    "    print(\"[SAVE]\", out_f)\n",
    "\n",
    "# 必要なら先頭だけ確認\n",
    "print(df_top10.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6bb6317-a9c8-49d4-8f91-b6aebcc07990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 特徴列数: 76\n",
      "[INFO] Ledoit-Wolf shrinkage を使用しました。\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_suitability_KMO_MSA.csv (76行×1列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_suitability_MAP_curve.csv (75行×1列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_suitability_corr_shrink.csv (76行×76列)\n",
      "[SAVE] /Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing/efa_suitability_summary.json\n",
      "{\n",
      "  \"n_samples\": 164,\n",
      "  \"n_variables\": 76,\n",
      "  \"detR\": 4.4407643684543186e-27,\n",
      "  \"avg_interitem_corr\": 0.013014797157139525,\n",
      "  \"KMO_overall\": 0.9322961982706411,\n",
      "  \"Bartlett_chi2\": 8302.905864366849,\n",
      "  \"Bartlett_p\": 0.0,\n",
      "  \"MAP_min\": 3.015448490864062e-06,\n",
      "  \"MAP_argmin_k\": 75\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from dataclasses import dataclass\n",
    "import json, os, traceback\n",
    "\n",
    "# ============ 既存と同じメタ列定義 ============\n",
    "ID_COL    = \"ID\"\n",
    "AGE_COL   = \"Age\"\n",
    "SEX_COL   = \"Gender\"\n",
    "SITE_COL  = \"site\"\n",
    "DIAG_COL  = \"diagnosis\"\n",
    "PART_COL  = \"Part\"\n",
    "\n",
    "META_DEFAULT_EXCLUDE = [ID_COL, AGE_COL, SEX_COL, SITE_COL, DIAG_COL, PART_COL]\n",
    "\n",
    "# ============ 保存系ユーティリティ ============\n",
    "def _ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _safe_csv(df: pd.DataFrame, path: str):\n",
    "    try:\n",
    "        df.to_csv(path, encoding=\"utf-8\", index=True)\n",
    "        print(f\"[SAVE] {path} ({df.shape[0]}行×{df.shape[1]}列)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CSV保存失敗: {path}\\n{e}\")\n",
    "\n",
    "def _safe_json(obj: dict, path: str):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[SAVE] {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] JSON保存失敗: {path}\\n{e}\")\n",
    "\n",
    "# ============ 前処理ユーティリティ ============\n",
    "def _select_feature_matrix(X: pd.DataFrame,\n",
    "                           exclude_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(X).copy()\n",
    "    # メタ除外\n",
    "    if exclude_cols:\n",
    "        drop_cols = [c for c in exclude_cols if c in X.columns]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols)\n",
    "            print(f\"[INFO] メタ列を除外: {drop_cols}\")\n",
    "    # 数値列のみ\n",
    "    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    X = X[num_cols].copy()\n",
    "    if len(X.columns) == 0:\n",
    "        raise ValueError(\"数値の特徴列がありません。入力にメタ列しかない可能性があります。\")\n",
    "    # ゼロ分散を除外\n",
    "    std = X.std(ddof=1)\n",
    "    zero_vars = std.index[std == 0].tolist()\n",
    "    if zero_vars:\n",
    "        X = X.drop(columns=zero_vars)\n",
    "        print(f\"[INFO] ゼロ分散列を除外: {zero_vars}\")\n",
    "    # 重複列を除外\n",
    "    dup = X.T.drop_duplicates().T  # 列ベースの重複除去\n",
    "    if dup.shape[1] != X.shape[1]:\n",
    "        kept = dup.columns.tolist()\n",
    "        removed = [c for c in X.columns if c not in kept]\n",
    "        print(f\"[INFO] 重複列を除外: {removed}\")\n",
    "        X = dup\n",
    "    print(f\"[INFO] 特徴列数: {X.shape[1]}\")\n",
    "    return X\n",
    "\n",
    "def _median_impute_zscore(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    for c in X.columns:\n",
    "        med = np.nanmedian(X[c].to_numpy(dtype=float))\n",
    "        X[c] = X[c].fillna(med)\n",
    "    Z = (X - X.mean()) / X.std(ddof=1).replace(0, np.nan)\n",
    "    return Z.fillna(0.0)\n",
    "\n",
    "def _find_high_correlation_cols(R: np.ndarray, cols: List[str], cutoff: float = 0.95) -> List[str]:\n",
    "    \"\"\"|r|がcutoff超の冗長列を返す（caret::findCorrelation相当の簡易版）\"\"\"\n",
    "    p = R.shape[0]\n",
    "    remove = set()\n",
    "    absR = np.abs(R.copy())\n",
    "    np.fill_diagonal(absR, 0.0)\n",
    "    while True:\n",
    "        i, j = divmod(absR.argmax(), p)\n",
    "        if absR[i, j] <= cutoff:\n",
    "            break\n",
    "        # 相関の平均が大きい方を落とす\n",
    "        mean_i = absR[i, :].mean()\n",
    "        mean_j = absR[j, :].mean()\n",
    "        drop = i if mean_i >= mean_j else j\n",
    "        remove.add(cols[drop])\n",
    "        absR[drop, :] = 0.0\n",
    "        absR[:, drop] = 0.0\n",
    "    return list(remove)\n",
    "\n",
    "# ============ 数値安定な相関行列づくり ============\n",
    "def _cor_shrink(Z: pd.DataFrame, ridge: float = 1e-6) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"縮小（shrink）＋リッジで正定値な相関行列を返す。sklearn が無ければリッジのみ。\"\"\"\n",
    "    cols = Z.columns.tolist()\n",
    "    X = Z.to_numpy(dtype=float)\n",
    "    # 相関\n",
    "    R = np.corrcoef(X, rowvar=False)\n",
    "    # まず冗長列の刈り込み（|r|>0.9999）\n",
    "    to_drop = _find_high_correlation_cols(R, cols, cutoff=0.9999)\n",
    "    if to_drop:\n",
    "        keep = [c for c in cols if c not in to_drop]\n",
    "        Z = Z[keep]\n",
    "        cols = keep\n",
    "        X = Z.to_numpy(dtype=float)\n",
    "        R = np.corrcoef(X, rowvar=False)\n",
    "        print(f\"[INFO] 極端な高相関列を除外: {to_drop}\")\n",
    "\n",
    "    # sklearn Ledoit-Wolf があれば使う\n",
    "    R_shrink = None\n",
    "    try:\n",
    "        from sklearn.covariance import LedoitWolf\n",
    "        lw = LedoitWolf().fit(X)\n",
    "        S = lw.covariance_\n",
    "        # 相関にスケーリング\n",
    "        d = np.sqrt(np.diag(S))\n",
    "        Dinv = np.diag(1.0 / np.where(d > 0, d, 1.0))\n",
    "        R_shrink = Dinv @ S @ Dinv\n",
    "        print(\"[INFO] Ledoit-Wolf shrinkage を使用しました。\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if R_shrink is None:\n",
    "        R_shrink = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "    # リッジで正定値化\n",
    "    eig = np.linalg.eigvalsh(R_shrink)\n",
    "    if eig.min() <= 1e-8:\n",
    "        eps = max(ridge, 1e-8 - eig.min() + 1e-8)\n",
    "        R_shrink = R_shrink + eps * np.eye(R_shrink.shape[0])\n",
    "        print(f\"[INFO] ridge={eps:.2e} を付加して正定値化しました。\")\n",
    "    return R_shrink, cols\n",
    "\n",
    "# ============ KMO（overall / per-variable） ============\n",
    "@dataclass\n",
    "class KMOResult:\n",
    "    overall: float\n",
    "    per_variable: Dict[str, float]\n",
    "\n",
    "def kmo_from_corr(R: np.ndarray, cols: List[str]) -> KMOResult:\n",
    "    # 逆相関（擬似逆行列も許容）\n",
    "    try:\n",
    "        invR = np.linalg.inv(R)\n",
    "    except np.linalg.LinAlgError:\n",
    "        invR = np.linalg.pinv(R)\n",
    "\n",
    "    # 偏相関行列\n",
    "    d = np.sqrt(np.diag(invR))\n",
    "    P = -invR / np.outer(d, d)\n",
    "    np.fill_diagonal(P, 0.0)\n",
    "\n",
    "    R2 = R**2\n",
    "    P2 = P**2\n",
    "    np.fill_diagonal(R2, 0.0)\n",
    "    np.fill_diagonal(P2, 0.0)\n",
    "\n",
    "    overall = R2.sum() / (R2.sum() + P2.sum() + 1e-12)\n",
    "\n",
    "    per_var = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        r2_i = np.delete(R2[i, :], i).sum()\n",
    "        p2_i = np.delete(P2[i, :], i).sum()\n",
    "        per_var[c] = float(r2_i / (r2_i + p2_i + 1e-12))\n",
    "    return KMOResult(overall=float(overall), per_variable=per_var)\n",
    "\n",
    "# ============ Bartlett’s test（相関が正定値でないと NaN） ============\n",
    "def bartlett_sphericity(R: np.ndarray, n: int) -> Tuple[float, float]:\n",
    "    p = R.shape[0]\n",
    "    sign, logdet = np.linalg.slogdet(R)\n",
    "    if not np.isfinite(logdet) or sign <= 0:\n",
    "        return (np.nan, np.nan)\n",
    "    chi2 = -(n - 1 - (2*p + 5)/6.0) * logdet\n",
    "    df = p*(p-1)/2.0\n",
    "    try:\n",
    "        from scipy.stats import chi2 as chi2dist\n",
    "        pval = 1.0 - chi2dist.cdf(chi2, df)\n",
    "    except Exception:\n",
    "        # SciPy がない環境でも値だけ返す\n",
    "        pval = np.nan\n",
    "    return float(chi2), float(pval)\n",
    "\n",
    "# ============ Velicer’s MAP（古典的MAP） ============\n",
    "def velicer_map_from_corr(R: np.ndarray) -> Tuple[np.ndarray, int, float]:\n",
    "    \"\"\"\n",
    "    R: 相関行列。対角=1 を仮定。\n",
    "    戻り値:\n",
    "      map_curve[k-1] = k成分をpartialしたときの平均二乗偏相関（対角除く）\n",
    "      k_opt, map_min\n",
    "    \"\"\"\n",
    "    # 固有分解\n",
    "    eigvals, eigvecs = np.linalg.eigh(R)\n",
    "    idx = eigvals.argsort()[::-1]\n",
    "    lam = eigvals[idx]\n",
    "    V = eigvecs[:, idx]\n",
    "    p = R.shape[0]\n",
    "    map_curve = np.full(p-1, np.nan, dtype=float)\n",
    "\n",
    "    for k in range(1, p):  # 1..p-1\n",
    "        Vk = V[:, :k]\n",
    "        Lk = Vk @ np.diag(np.sqrt(lam[:k]))   # PCA loadings\n",
    "        Rk = Lk @ Lk.T\n",
    "        resid = R - Rk\n",
    "        np.fill_diagonal(resid, 0.0)\n",
    "        map_curve[k-1] = np.mean(resid**2)\n",
    "\n",
    "    k_opt = int(np.nanargmin(map_curve) + 1)\n",
    "    map_min = float(np.nanmin(map_curve))\n",
    "    return map_curve, k_opt, map_min\n",
    "\n",
    "# ============ メイン：適性指標の一括算出 ============\n",
    "def compute_fa_suitability(\n",
    "    X_in: pd.DataFrame,\n",
    "    exclude_cols: Optional[List[str]] = META_DEFAULT_EXCLUDE,\n",
    "    output_dir: Optional[str] = None,\n",
    "    file_prefix: str = \"efa_suitability\"\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    X_in: 行=サブジェクト、列=メタ+特徴（Train×ID平均後の行列を推奨）\n",
    "    \"\"\"\n",
    "    X_feat = _select_feature_matrix(X_in, exclude_cols=exclude_cols)\n",
    "    Z = _median_impute_zscore(X_feat)            # 中央値補完→標準化\n",
    "    R_pd, cols = _cor_shrink(Z)                  # 正定値な相関（縮小＋必要ならridge）\n",
    "\n",
    "    # 指標\n",
    "    kmo = kmo_from_corr(R_pd, cols)\n",
    "    chi2, pval = bartlett_sphericity(R_pd, n=Z.shape[0])\n",
    "    detR = float(np.linalg.det(R_pd))\n",
    "    avg_r = float(np.mean(R_pd[np.triu_indices_from(R_pd, k=1)]))\n",
    "    map_curve, map_k, map_min = velicer_map_from_corr(R_pd)\n",
    "\n",
    "    summary = {\n",
    "        \"n_samples\": int(Z.shape[0]),\n",
    "        \"n_variables\": int(Z.shape[1]),\n",
    "        \"detR\": detR,\n",
    "        \"avg_interitem_corr\": avg_r,\n",
    "        \"KMO_overall\": kmo.overall,\n",
    "        \"Bartlett_chi2\": chi2,\n",
    "        \"Bartlett_p\": pval,\n",
    "        \"MAP_min\": map_min,\n",
    "        \"MAP_argmin_k\": int(map_k),\n",
    "    }\n",
    "\n",
    "    if output_dir:\n",
    "        _ensure_dir(output_dir)\n",
    "        _safe_csv(pd.DataFrame({\"variable\": list(kmo.per_variable.keys()),\n",
    "                                \"MSA\": list(kmo.per_variable.values())}).set_index(\"variable\"),\n",
    "                  os.path.join(output_dir, f\"{file_prefix}_KMO_MSA.csv\"))\n",
    "        _safe_csv(pd.DataFrame({\"k\": np.arange(1, len(map_curve)+1),\n",
    "                                \"MAP\": map_curve}).set_index(\"k\"),\n",
    "                  os.path.join(output_dir, f\"{file_prefix}_MAP_curve.csv\"))\n",
    "        _safe_csv(pd.DataFrame(R_pd, index=cols, columns=cols),\n",
    "                  os.path.join(output_dir, f\"{file_prefix}_corr_shrink.csv\"))\n",
    "        _safe_json(summary, os.path.join(output_dir, f\"{file_prefix}_summary.json\"))\n",
    "\n",
    "    return summary\n",
    "\n",
    "# ==============================\n",
    "# 例：フォルダのCSVから実行\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        OUTPUT_DIR = \"/Users/psychetmdu/NLP/pipeline_nlp/2_2_datasheet_ratio_normalized/p1_preprocessing\"  # <- 変更してください\n",
    "        x_path = os.path.join(OUTPUT_DIR, \"efa_X_train_subject.csv\")  # 既存パイプラインの出力\n",
    "        X_in = pd.read_csv(x_path, index_col=0)\n",
    "        summary = compute_fa_suitability(X_in, exclude_cols=META_DEFAULT_EXCLUDE,\n",
    "                                         output_dir=OUTPUT_DIR, file_prefix=\"efa_suitability\")\n",
    "        print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] 適性指標の算出に失敗:\", e)\n",
    "        print(traceback.format_exc(limit=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5fe6d20-f4f3-4402-9c99-080d03fb8368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] counts:\n",
      "diagnosis  0.0  1.0\n",
      "site               \n",
      "A            0   26\n",
      "B            0   13\n",
      "G            0   39\n",
      "K          153   44\n",
      "M            0   18\n",
      "N            2    5\n",
      "O            0   38\n",
      "T            0   25\n",
      "[Train] row-proportions:\n",
      "diagnosis   0.0    1.0\n",
      "site                  \n",
      "A           0.0  100.0\n",
      "B           0.0  100.0\n",
      "G           0.0  100.0\n",
      "K          77.7   22.3\n",
      "M           0.0  100.0\n",
      "N          28.6   71.4\n",
      "O           0.0  100.0\n",
      "T           0.0  100.0\n",
      "\n",
      "[Test] counts:\n",
      "diagnosis  0.0  1.0\n",
      "site               \n",
      "A            0   14\n",
      "G            0    5\n",
      "K           38   15\n",
      "M            0    4\n",
      "O            0    9\n",
      "[Test] row-proportions:\n",
      "diagnosis   0.0    1.0\n",
      "site                  \n",
      "A           0.0  100.0\n",
      "G           0.0  100.0\n",
      "K          71.7   28.3\n",
      "M           0.0  100.0\n",
      "O           0.0  100.0\n"
     ]
    }
   ],
   "source": [
    "# site × 診断のクロス集計（Train/Test別）\n",
    "def cross_tab_site_diag(df, site_col=\"site\", diag_col=\"diagnosis\", part_col=\"partition_P1\"):\n",
    "    out = {}\n",
    "    for split in [\"Train\",\"Test\"]:\n",
    "        sub = df[df[part_col]==split]\n",
    "        tab = pd.crosstab(sub[site_col], sub[diag_col], dropna=False)\n",
    "        prop = tab.div(tab.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "        out[split] = (tab, prop)\n",
    "        print(f\"\\n[{split}] counts:\\n{tab}\\n[{split}] row-proportions:\\n{(prop*100).round(1)}\")\n",
    "    return out\n",
    "\n",
    "# 例：df は前処理前の生データ（診断はSCZ/HCでも0/1でもOK）\n",
    "cross = cross_tab_site_diag(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca9920-7e08-4095-9dcb-1e99389849aa",
   "metadata": {},
   "source": [
    "# EFA Validation: Parallel Analysis / Bootstrap / Monte Carlo\n",
    "\n",
    "このセクションでは、因子分析（EFA）の **因子数の妥当性** と **因子負荷・代表特徴の安定性** を、\n",
    "1) Monte Carlo型の**並行分析（Parallel Analysis）**、\n",
    "2) **ブートストラップ** および \n",
    "3) **サブサンプリング型Monte Carlo** により検証します。\n",
    "\n",
    "- 解析対象はトレーニングデータ（情報リーク防止）とします。\n",
    "- `X_train`（形状: サンプル×特徴量）の行列（numpy配列またはDataFrame）を入力として想定します。\n",
    "- 必要に応じて、下の「USER INPUT」欄で変数名を調整してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a26c3b-b359-4959-986a-dfdd0d7ccc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ID平均: n_subjects=164, n_features=76\n",
      "[SAVE] ./efa_pipeline_validation/diag_top_variances_pre_standardize.csv\n",
      "[PA] 推奨k=9  (pct=99.0, iter=2000)\n",
      "[SAVE] ./efa_pipeline_validation/pa_corr.png\n",
      "[SAVE] ./efa_pipeline_validation/loadings_raw.csv\n",
      "[SAVE] ./efa_pipeline_validation/loadings_varimax.csv\n",
      "[SAVE] ./efa_pipeline_validation/efa_loadings.csv\n",
      "[SAVE] ./efa_pipeline_validation/raw_top_features.csv\n",
      "[SAVE] ./efa_pipeline_validation/varimax_top_features.csv\n",
      "[SAVE] representatives*.csv/json/log -> ./efa_pipeline_validation\n",
      "[REP] Varimax＋一意化の代表（top1/因子）: ['num_INTJ_ratio', 'num_ADV_ratio', 'total_tree_height_CEJCminus_ratio', 'num_shuujoshi_ratio', 'num_sentences', 'w2Vsim_mean', 'obj_ratio', 'nmod_ratio', 'num_rentaishi_ratio']\n",
      "[FIT] 残差相関: max|r|=0.5776, Fro=3.6711\n",
      "[SAVE] ./efa_pipeline_validation/residual_corr_matrix.csv\n",
      "[SAVE] ./efa_pipeline_validation/top_residual_pairs_varimax.csv\n",
      "[SAVE] ./efa_pipeline_validation/bootstrap_varimax_rep_freq_top5.csv\n",
      "[SAVE] ./efa_pipeline_validation/mcsub_varimax_rep_freq_top5.csv\n",
      "[SAVE] ./efa_pipeline_validation/stability_varimax.png\n",
      "[SAVE] ./efa_pipeline_validation/summary_pipeline_validation.json\n",
      "\n",
      "=== 出力物 ===\n",
      "- efa_loadings_csv: ./efa_pipeline_validation/efa_loadings.csv\n",
      "- representatives_csv: ./efa_pipeline_validation/representatives.csv\n",
      "- representatives_log: ./efa_pipeline_validation/representatives_selection_log.csv\n",
      "- top10_varimax: ./efa_pipeline_validation/varimax_top_features.csv\n",
      "- bootstrap_rep_freq_top5: ./efa_pipeline_validation/bootstrap_varimax_rep_freq_top5.csv\n",
      "- mc_rep_freq_top5: ./efa_pipeline_validation/mcsub_varimax_rep_freq_top5.csv\n",
      "- stability_png: ./efa_pipeline_validation/stability_varimax.png\n",
      "- residual_matrix: ./efa_pipeline_validation/residual_corr_matrix.csv\n",
      "- residual_pairs: ./efa_pipeline_validation/top_residual_pairs_varimax.csv\n",
      "- pa_plot: ./efa_pipeline_validation/pa_corr.png\n"
     ]
    }
   ],
   "source": [
    "# 完全版コード\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# 実運用パイプライン準拠：EFA + Varimax + 代表選定 + PA(k決定) + 残差診断\n",
    "#                           ＋ 安定性検証（Bootstrap / MCサブサンプリング）\n",
    "# 必要: train_processed, pp_params['feature_cols'], ID_COL（無ければ 'ID'）\n",
    "# 出力: ./efa_pipeline_validation 以下にCSV/PNG/JSONを保存\n",
    "# =========================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import eigh, inv\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# -------------------------------\n",
    "# 設定\n",
    "# -------------------------------\n",
    "OUTDIR = \"./efa_pipeline_validation\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 乱数・反復\n",
    "SEED_FA  = 123\n",
    "SEED_PA  = 42\n",
    "SEED_BOOT= 456\n",
    "SEED_MC  = 789\n",
    "N_ITER_PA= 2000\n",
    "PCT_PA   = 99.0\n",
    "N_BOOT   = 2000\n",
    "N_MC     = 2000\n",
    "SS_RATIO = 0.70\n",
    "\n",
    "# 代表選定ポリシー（本文ポリシー）\n",
    "MIN_LOADING = 0.60\n",
    "MIN_GAP     = 0.20\n",
    "CROSS_MAX   = 0.30\n",
    "MAX_ABS_R   = 0.80   # 代表間の相関一意化\n",
    "\n",
    "# -------------------------------\n",
    "# 入力チェック & ID平均\n",
    "# -------------------------------\n",
    "ID_COL = globals().get(\"ID_COL\", \"ID\")\n",
    "if 'train_processed' not in globals() or 'pp_params' not in globals():\n",
    "    raise ValueError(\"train_processed / pp_params が見つかりません。\")\n",
    "feat_cols = [c for c in pp_params.get(\"feature_cols\", []) if c in train_processed.columns]\n",
    "if len(feat_cols) == 0:\n",
    "    raise ValueError(\"pp_params['feature_cols'] が空、または列が見つかりません。\")\n",
    "if ID_COL not in train_processed.columns:\n",
    "    raise ValueError(f\"ID列 {ID_COL!r} が train_processed に存在しません。\")\n",
    "\n",
    "X_id_mean: pd.DataFrame = (\n",
    "    train_processed[[ID_COL] + feat_cols]\n",
    "    .dropna(subset=[ID_COL])\n",
    "    .groupby(ID_COL)[feat_cols].mean()\n",
    ")\n",
    "\n",
    "# ゼロ分散除外\n",
    "std0 = X_id_mean.std(ddof=1)\n",
    "zero_vars = std0.index[std0 == 0].tolist()\n",
    "if zero_vars:\n",
    "    X_id_mean = X_id_mean.drop(columns=zero_vars)\n",
    "    print(f\"[INFO] ゼロ分散列を除外: {zero_vars}\")\n",
    "\n",
    "print(f\"[INFO] ID平均: n_subjects={X_id_mean.shape[0]}, n_features={X_id_mean.shape[1]}\")\n",
    "\n",
    "# 標準化（列z）\n",
    "mu = X_id_mean.mean(axis=0)\n",
    "sd = X_id_mean.std(axis=0, ddof=1).replace(0, np.nan)\n",
    "Xz = (X_id_mean - mu) / sd\n",
    "Xz = Xz.fillna(0.0)\n",
    "\n",
    "# 参考：標準化前の分散トップ\n",
    "X_id_mean.var(ddof=1).sort_values(ascending=False).head(10)\\\n",
    "    .to_csv(os.path.join(OUTDIR, \"diag_top_variances_pre_standardize.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"diag_top_variances_pre_standardize.csv\"))\n",
    "\n",
    "# 相関行列\n",
    "R = Xz.corr().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "np.fill_diagonal(R.values, 1.0)\n",
    "\n",
    "# -------------------------------\n",
    "# Horn 型 Parallel Analysis（相関ベース）で k を決定\n",
    "# -------------------------------\n",
    "def _cov_eigvals_from_array(X_arr: np.ndarray) -> np.ndarray:\n",
    "    Xn = (X_arr - X_arr.mean(0, keepdims=True)) / (X_arr.std(0, keepdims=True) + 1e-12)\n",
    "    C = np.cov(Xn, rowvar=False)\n",
    "    C = (C + C.T) * 0.5\n",
    "    vals, _ = eigh(C)\n",
    "    return vals[::-1]\n",
    "\n",
    "def parallel_analysis_corr(X_df: pd.DataFrame, n_iter=N_ITER_PA, pct=PCT_PA, seed=SEED_PA) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    n, p = X_arr.shape\n",
    "    obs = _cov_eigvals_from_array(X_arr)\n",
    "    null_eigs = np.empty((n_iter, p), float)\n",
    "    for i in range(n_iter):\n",
    "        Z = rng.standard_normal(size=(n, p))\n",
    "        null_eigs[i] = _cov_eigvals_from_array(Z)\n",
    "    thr = np.percentile(null_eigs, pct, axis=0)\n",
    "    k = int(np.sum(obs > thr))\n",
    "    return {\"obs\": obs, \"thr\": thr, \"k\": k}\n",
    "\n",
    "pa = parallel_analysis_corr(Xz, n_iter=N_ITER_PA, pct=PCT_PA, seed=SEED_PA)\n",
    "k_final = max(1, pa[\"k\"])\n",
    "print(f\"[PA] 推奨k={k_final}  (pct={PCT_PA}, iter={N_ITER_PA})\")\n",
    "\n",
    "# 図保存\n",
    "def _save_pa_plot(obs, thr, k, path, title):\n",
    "    x = np.arange(1, len(obs)+1)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x, obs, marker='o', label='Observed eigs')\n",
    "    plt.plot(x, thr, marker='x', linestyle='--', label=f'Null {int(PCT_PA)}th pct')\n",
    "    if k>0:\n",
    "        plt.axvline(k, color='gray', linestyle=':')\n",
    "        plt.text(k+0.3, float(np.max(obs))*0.95, f'k={k}', ha='left', va='top')\n",
    "    plt.xlabel('Component index'); plt.ylabel('Eigenvalue')\n",
    "    plt.title(title); plt.legend(); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    print(\"[SAVE]\", path)\n",
    "\n",
    "_save_pa_plot(pa[\"obs\"], pa[\"thr\"], pa[\"k\"], os.path.join(OUTDIR, \"pa_corr.png\"),\n",
    "              \"Parallel Analysis (Correlation-based, Horn)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 因子分析（sklearn FA, EM）→ Varimax回転 → 負荷/上位/代表\n",
    "# -------------------------------\n",
    "def fit_fa_return_loadings_df(X_df: pd.DataFrame, n_factors: int, seed: int = SEED_FA) -> Tuple[pd.DataFrame, np.ndarray, FactorAnalysis]:\n",
    "    fa = FactorAnalysis(n_components=n_factors, random_state=seed)\n",
    "    W = fa.fit(X_df.values.astype(float)).components_.T  # (p x k)\n",
    "    cols = [f\"Factor{j+1}\" for j in range(n_factors)]\n",
    "    return pd.DataFrame(W, index=X_df.columns, columns=cols), fa.noise_variance_, fa\n",
    "\n",
    "def varimax(W: np.ndarray, gamma: float = 1.0, q: int = 20, tol: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    p, k = W.shape\n",
    "    Rm = np.eye(k); d = 0\n",
    "    for _ in range(q):\n",
    "        d0 = d\n",
    "        Lam = W @ Rm\n",
    "        u, s, vh = np.linalg.svd(\n",
    "            W.T @ (Lam**3 - (gamma/p) * Lam @ np.diag(np.sum(Lam**2, axis=0))),\n",
    "            full_matrices=False\n",
    "        )\n",
    "        Rm = u @ vh\n",
    "        d = np.sum(s)\n",
    "        if d0 != 0 and d/d0 < 1 + tol:\n",
    "            break\n",
    "    return W @ Rm, Rm  # (rotated, rotation matrix)\n",
    "\n",
    "def top_features_per_factor(load_df: pd.DataFrame, topn: int = 10) -> Dict[str, pd.DataFrame]:\n",
    "    res = {}\n",
    "    for c in load_df.columns:\n",
    "        s = load_df[c].abs().sort_values(ascending=False).head(topn)\n",
    "        res[c] = pd.DataFrame({\"feature\": s.index, \"abs_loading\": s.values})\n",
    "    return res\n",
    "\n",
    "def save_top_tables(tops: Dict[str, pd.DataFrame], prefix: str):\n",
    "    tbls = []\n",
    "    for j, (c, df) in enumerate(tops.items(), start=1):\n",
    "        df2 = df.copy()\n",
    "        df2.columns = [f\"Factor{j}_feature\", f\"Factor{j}_absload\"]\n",
    "        tbls.append(df2.reset_index(drop=True))\n",
    "    big = pd.concat(tbls, axis=1)\n",
    "    big.to_csv(os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(\"[SAVE]\", os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"))\n",
    "\n",
    "def select_reps_with_policy(load_df: pd.DataFrame, R_df: pd.DataFrame,\n",
    "                            min_loading: float = MIN_LOADING, min_gap: float = MIN_GAP,\n",
    "                            cross_max: float = CROSS_MAX, max_abs_r: float = MAX_ABS_R\n",
    "                           ) -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"本文の代表選定ポリシー：|λ|≥min_loading, gap≥min_gap, cross_max≤cross_max, 代表間|r|<max_abs_r。\n",
    "       充足候補が無い因子は段階的に緩和（gap→cross→loading→相関閾値）して必ず1本選ぶ。\"\"\"\n",
    "    fac_cols = list(load_df.columns)\n",
    "    Labs = load_df.abs()\n",
    "    selected, rows = [], []\n",
    "    for f in fac_cols:\n",
    "        ser = Labs[f].sort_values(ascending=False)\n",
    "        chosen = None; decision = \"strict\"\n",
    "        # strict\n",
    "        for feat, lam in ser.items():\n",
    "            if feat in selected: continue\n",
    "            others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "            max_other = float(others.max()) if len(others) else 0.0\n",
    "            gap = float(lam - max_other)\n",
    "            if lam >= min_loading and gap >= min_gap and max_other <= cross_max and \\\n",
    "               all(abs(R_df.loc[feat, r]) <= max_abs_r for r in selected):\n",
    "                chosen = (feat, float(load_df.loc[feat, f]), lam, max_other, gap)\n",
    "                break\n",
    "        # relax stages\n",
    "        stages = [\n",
    "            (\"relax_gap\",   (min_loading, 0.10, cross_max, max_abs_r)),\n",
    "            (\"relax_cross\", (min_loading, 0.10, 1.00,     max_abs_r)),\n",
    "            (\"relax_load\",  (0.00,        0.00, 1.00,     max_abs_r)),\n",
    "            (\"relax_corr\",  (0.00,        0.00, 1.00,     0.95))\n",
    "        ]\n",
    "        if chosen is None:\n",
    "            for name, (ml, mg, cx, rr) in stages:\n",
    "                for feat, lam in ser.items():\n",
    "                    if feat in selected: continue\n",
    "                    others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "                    max_other = float(others.max()) if len(others) else 0.0\n",
    "                    gap = float(lam - max_other)\n",
    "                    if lam >= ml and gap >= mg and max_other <= cx and \\\n",
    "                       all(abs(R_df.loc[feat, r]) <= rr for r in selected):\n",
    "                        chosen = (feat, float(load_df.loc[feat, f]), lam, max_other, gap)\n",
    "                        decision = name\n",
    "                        break\n",
    "                if chosen is not None:\n",
    "                    break\n",
    "        if chosen is None:\n",
    "            # 最終手段：未使用の最大\n",
    "            feat = ser.index[0]\n",
    "            lam = float(ser.iloc[0]); lam_signed = float(load_df.loc[feat, f])\n",
    "            others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "            max_other = float(others.max()) if len(others) else 0.0\n",
    "            gap = float(lam - max_other)\n",
    "            chosen = (feat, lam_signed, lam, max_other, gap)\n",
    "            decision = \"fallback_max\"\n",
    "        feat, lam_signed, lam_abs, max_other, gap = chosen\n",
    "        selected.append(feat)\n",
    "        rows.append({\n",
    "            \"factor\": f, \"feature\": feat, \"loading\": lam_signed, \"abs_loading\": lam_abs,\n",
    "            \"max_other_abs\": max_other, \"gap_abs\": gap, \"decision\": decision\n",
    "        })\n",
    "    log_df = pd.DataFrame(rows)\n",
    "    return selected, log_df\n",
    "\n",
    "# --- FA fit & rotate ---\n",
    "load_raw, psi_raw, fa_model = fit_fa_return_loadings_df(Xz, k_final, seed=SEED_FA)\n",
    "W_rot, _ = varimax(load_raw.values.copy())\n",
    "load_varimax = pd.DataFrame(W_rot, index=load_raw.index, columns=load_raw.columns)\n",
    "\n",
    "# 保存（efa_loadings.csv も出す）\n",
    "load_raw.to_csv(os.path.join(OUTDIR, \"loadings_raw.csv\"), encoding=\"utf-8\")\n",
    "load_varimax.to_csv(os.path.join(OUTDIR, \"loadings_varimax.csv\"), encoding=\"utf-8\")\n",
    "load_varimax.to_csv(os.path.join(OUTDIR, \"efa_loadings.csv\"), encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"loadings_raw.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"loadings_varimax.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"efa_loadings.csv\"))\n",
    "\n",
    "# 各因子 Top10\n",
    "tops_raw = top_features_per_factor(load_raw, topn=10)\n",
    "tops_rot = top_features_per_factor(load_varimax, topn=10)\n",
    "def _save_top(prefix, tops):\n",
    "    tbls = []\n",
    "    for j, (c, df) in enumerate(tops.items(), start=1):\n",
    "        d = df.copy(); d.columns=[f\"Factor{j}_feature\", f\"Factor{j}_absload\"]\n",
    "        tbls.append(d.reset_index(drop=True))\n",
    "    big = pd.concat(tbls, axis=1)\n",
    "    big.to_csv(os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(\"[SAVE]\", os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"))\n",
    "_save_top(\"raw\", tops_raw)\n",
    "_save_top(\"varimax\", tops_rot)\n",
    "\n",
    "# 代表選定（本文ポリシー）\n",
    "reps, rep_log = select_reps_with_policy(load_varimax, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "rep_log.to_csv(os.path.join(OUTDIR, \"representatives_selection_log.csv\"), index=False, encoding=\"utf-8\")\n",
    "pd.DataFrame({\"feature\": reps, \"factor\": [f\"Factor{i+1}\" for i in range(len(reps))]})\\\n",
    "  .to_csv(os.path.join(OUTDIR, \"representatives.csv\"), index=False, encoding=\"utf-8\")\n",
    "with open(os.path.join(OUTDIR, \"representatives.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"representatives\": reps}, f, ensure_ascii=False, indent=2)\n",
    "print(\"[SAVE] representatives*.csv/json/log ->\", OUTDIR)\n",
    "print(\"[REP] Varimax＋一意化の代表（top1/因子）:\", reps)\n",
    "\n",
    "# -------------------------------\n",
    "# 適合度診断（回転後負荷での再現相関）\n",
    "# -------------------------------\n",
    "def reproduced_corr_from_loadings(W: np.ndarray, psi: np.ndarray) -> np.ndarray:\n",
    "    R_hat = W @ W.T + np.diag(psi)\n",
    "    # 対角=1に補正（標準化空間）\n",
    "    for i in range(R_hat.shape[0]):\n",
    "        R_hat[i, i] = 1.0\n",
    "    return R_hat\n",
    "\n",
    "R_hat = reproduced_corr_from_loadings(load_varimax.values, psi_raw)\n",
    "resid = R.values - R_hat\n",
    "max_abs_resid = float(np.max(np.abs(resid)))\n",
    "frob = float(np.linalg.norm(resid, \"fro\"))\n",
    "np.savetxt(os.path.join(OUTDIR, \"residual_corr_matrix.csv\"), resid, delimiter=\",\")\n",
    "print(f\"[FIT] 残差相関: max|r|={max_abs_resid:.4f}, Fro={frob:.4f}\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"residual_corr_matrix.csv\"))\n",
    "\n",
    "# 上位残差ペア（冗長候補の確認用）\n",
    "p = len(load_varimax.index)\n",
    "tri_idx = [(i,j) for i in range(p) for j in range(i+1, p)]\n",
    "tri_vals = [abs(resid[i,j]) for (i,j) in tri_idx]\n",
    "order = np.argsort(tri_vals)[::-1][:30]\n",
    "rows = []\n",
    "feat_names = list(load_varimax.index)\n",
    "for idx in order:\n",
    "    i,j = tri_idx[idx]\n",
    "    rows.append({\n",
    "        \"feature_i\": feat_names[i], \"feature_j\": feat_names[j],\n",
    "        \"abs_residual_corr\": float(abs(resid[i,j])),\n",
    "        \"residual_corr\": float(resid[i,j]),\n",
    "        \"raw_corr\": float(R.values[i,j])\n",
    "    })\n",
    "pd.DataFrame(rows).to_csv(os.path.join(OUTDIR, \"top_residual_pairs_varimax.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"top_residual_pairs_varimax.csv\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 安定性検証（Bootstrap / MC）※パイプラインと同一手順を厳守\n",
    "# -------------------------------\n",
    "def orthogonal_procrustes_align(W_ref: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
    "    U, _, Vt = np.linalg.svd(W.T @ W_ref, full_matrices=False)\n",
    "    Q = U @ Vt\n",
    "    return W @ Q\n",
    "\n",
    "def tucker_congruence(W1: np.ndarray, W2: np.ndarray) -> float:\n",
    "    num = (W1 * W2).sum(axis=0)\n",
    "    den = np.sqrt((W1**2).sum(axis=0) * (W2**2).sum(axis=0)) + 1e-12\n",
    "    return float(np.mean(num / den))\n",
    "\n",
    "def _rep_freq_to_df(rep_counts: List[Dict[str,int]], topn: int = 5) -> pd.DataFrame:\n",
    "    tables = []\n",
    "    for j, cnt in enumerate(rep_counts, start=1):\n",
    "        total = sum(cnt.values()) if len(cnt) else 1\n",
    "        freq = {k: v/total for k,v in cnt.items()}\n",
    "        top = sorted(freq.items(), key=lambda kv: -kv[1])[:topn]\n",
    "        tables.append(pd.DataFrame(top, columns=[f\"Factor{j}_feature\", \"freq\"]).reset_index(drop=True))\n",
    "    return pd.concat(tables, axis=1)\n",
    "\n",
    "def stability_bootstrap(X_df: pd.DataFrame, k: int, n_boot=N_BOOT, seed=SEED_BOOT) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 参照（Full-data）：FA→Varimax→代表（本文ポリシー）\n",
    "    load0, psi0, _ = fit_fa_return_loadings_df(X_df, k, seed=rng.integers(1e9))\n",
    "    Wref, _ = varimax(load0.values)\n",
    "    load_ref = pd.DataFrame(Wref, index=load0.index, columns=load0.columns)\n",
    "    reps_ref, _ = select_reps_with_policy(load_ref, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "\n",
    "    congr = []; rep_counts = [dict() for _ in range(k)]\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    for _ in range(n_boot):\n",
    "        Xb = resample(X_arr, replace=True, n_samples=X_arr.shape[0], random_state=rng.integers(1e9))\n",
    "        load_b, _, _ = fit_fa_return_loadings_df(pd.DataFrame(Xb, columns=X_df.columns), k, seed=rng.integers(1e9))\n",
    "        Wb, _ = varimax(load_b.values)\n",
    "        Wb_al = orthogonal_procrustes_align(Wref, Wb)\n",
    "        congr.append(tucker_congruence(Wref, Wb_al))\n",
    "        reps_b, _ = select_reps_with_policy(pd.DataFrame(Wb_al, index=load_b.index, columns=load_b.columns),\n",
    "                                            R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "        for j, name in enumerate(reps_b):\n",
    "            rep_counts[j][name] = rep_counts[j].get(name, 0) + 1\n",
    "\n",
    "    rep_top5 = _rep_freq_to_df(rep_counts, topn=5)\n",
    "    return {\n",
    "        \"tucker_mean\": float(np.mean(congr)),\n",
    "        \"tucker_sd\":   float(np.std(congr, ddof=1)),\n",
    "        \"rep_counts\":  rep_counts,\n",
    "        \"rep_top5\":    rep_top5,\n",
    "        \"reps_ref\":    reps_ref\n",
    "    }\n",
    "\n",
    "def stability_mc_subsample(X_df: pd.DataFrame, k: int, n_iter=N_MC, ratio=SS_RATIO, seed=SEED_MC) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 参照\n",
    "    load0, psi0, _ = fit_fa_return_loadings_df(X_df, k, seed=rng.integers(1e9))\n",
    "    Wref, _ = varimax(load0.values)\n",
    "    load_ref = pd.DataFrame(Wref, index=load0.index, columns=load0.columns)\n",
    "    reps_ref, _ = select_reps_with_policy(load_ref, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "\n",
    "    congr = []; rep_counts = [dict() for _ in range(k)]\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    n = X_arr.shape[0]; ss = max(2, int(round(n * ratio)))\n",
    "    for _ in range(n_iter):\n",
    "        idx = rng.choice(n, size=ss, replace=False)\n",
    "        Xi = X_arr[idx]\n",
    "        load_i, _, _ = fit_fa_return_loadings_df(pd.DataFrame(Xi, columns=X_df.columns), k, seed=rng.integers(1e9))\n",
    "        Wi, _ = varimax(load_i.values)\n",
    "        Wi_al = orthogonal_procrustes_align(Wref, Wi)\n",
    "        congr.append(tucker_congruence(Wref, Wi_al))\n",
    "        reps_i, _ = select_reps_with_policy(pd.DataFrame(Wi_al, index=load_i.index, columns=load_i.columns),\n",
    "                                            R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "        for j, name in enumerate(reps_i):\n",
    "            rep_counts[j][name] = rep_counts[j].get(name, 0) + 1\n",
    "\n",
    "    rep_top5 = _rep_freq_to_df(rep_counts, topn=5)\n",
    "    return {\n",
    "        \"tucker_mean\": float(np.mean(congr)),\n",
    "        \"tucker_sd\":   float(np.std(congr, ddof=1)),\n",
    "        \"rep_counts\":  rep_counts,\n",
    "        \"rep_top5\":    rep_top5,\n",
    "        \"reps_ref\":    reps_ref\n",
    "    }\n",
    "\n",
    "# 実行\n",
    "boot = stability_bootstrap(Xz, k_final, n_boot=N_BOOT, seed=SEED_BOOT)\n",
    "mc   = stability_mc_subsample(Xz, k_final, n_iter=N_MC, ratio=SS_RATIO, seed=SEED_MC)\n",
    "\n",
    "# 保存（頻度Top5、棒グラフ）\n",
    "boot[\"rep_top5\"].to_csv(os.path.join(OUTDIR, \"bootstrap_varimax_rep_freq_top5.csv\"), index=False, encoding=\"utf-8\")\n",
    "mc[\"rep_top5\"].to_csv(os.path.join(OUTDIR, \"mcsub_varimax_rep_freq_top5.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"bootstrap_varimax_rep_freq_top5.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"mcsub_varimax_rep_freq_top5.csv\"))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([0,1], [boot[\"tucker_mean\"], mc[\"tucker_mean\"]])\n",
    "plt.xticks([0,1], ['Bootstrap', 'MC-Subsample'])\n",
    "plt.ylabel(\"Tucker's congruence (mean)\")\n",
    "plt.title('Stability: varimax (pipeline-conformant)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"stability_varimax.png\"), dpi=150); plt.close()\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"stability_varimax.png\"))\n",
    "\n",
    "# -------------------------------\n",
    "# サマリ保存\n",
    "# -------------------------------\n",
    "summary = {\n",
    "    \"n_subjects\": int(Xz.shape[0]),\n",
    "    \"n_features\": int(Xz.shape[1]),\n",
    "    \"k_final\": int(k_final),\n",
    "    \"pa\": {\"pct\": PCT_PA, \"n_iter\": N_ITER_PA, \"k\": int(pa[\"k\"])},\n",
    "    \"fa\": {\"seed\": SEED_FA},\n",
    "    \"representatives\": reps,\n",
    "    \"fit\": {\"max_abs_resid\": max_abs_resid, \"frob\": frob},\n",
    "    \"stability\": {\n",
    "        \"bootstrap\": {\"tucker_mean\": boot[\"tucker_mean\"], \"tucker_sd\": boot[\"tucker_sd\"]},\n",
    "        \"mc\":        {\"tucker_mean\": mc[\"tucker_mean\"],   \"tucker_sd\": mc[\"tucker_sd\"]}\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"min_loading\": MIN_LOADING, \"min_gap\": MIN_GAP, \"cross_max\": CROSS_MAX, \"max_abs_r\": MAX_ABS_R\n",
    "    },\n",
    "    \"seeds\": {\"PA\": SEED_PA, \"FA\": SEED_FA, \"BOOT\": SEED_BOOT, \"MC\": SEED_MC},\n",
    "    \"files\": {\n",
    "        \"efa_loadings_csv\": \"efa_loadings.csv\",\n",
    "        \"representatives_csv\": \"representatives.csv\",\n",
    "        \"representatives_log\": \"representatives_selection_log.csv\",\n",
    "        \"top10_varimax\": \"varimax_top_features.csv\",\n",
    "        \"bootstrap_rep_freq_top5\": \"bootstrap_varimax_rep_freq_top5.csv\",\n",
    "        \"mc_rep_freq_top5\": \"mcsub_varimax_rep_freq_top5.csv\",\n",
    "        \"stability_png\": \"stability_varimax.png\",\n",
    "        \"residual_matrix\": \"residual_corr_matrix.csv\",\n",
    "        \"residual_pairs\": \"top_residual_pairs_varimax.csv\",\n",
    "        \"pa_plot\": \"pa_corr.png\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUTDIR, \"summary_pipeline_validation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"summary_pipeline_validation.json\"))\n",
    "\n",
    "print(\"\\n=== 出力物 ===\")\n",
    "for k,v in summary[\"files\"].items():\n",
    "    print(f\"- {k}: {os.path.join(OUTDIR, v)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4da67a-e3ef-436a-8a1a-c53ae2dd5517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c637cc-c4f8-4f4b-9a9e-dee42d3a8651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "148edb0e-6ae4-4ea0-b6ce-d8f17a5332b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ID平均: n_subjects=164, n_features=76\n",
      "[SAVE] ./efa_pipeline_validation/diag_top_variances_pre_standardize.csv\n",
      "[PA] 推奨k=9  (pct=99.0, iter=2000)\n",
      "[SAVE] ./efa_pipeline_validation/pa_corr.png\n",
      "[SAVE] ./efa_pipeline_validation/loadings_raw.csv\n",
      "[SAVE] ./efa_pipeline_validation/loadings_varimax.csv\n",
      "[SAVE] ./efa_pipeline_validation/efa_loadings.csv\n",
      "[SAVE] ./efa_pipeline_validation/raw_top_features.csv\n",
      "[SAVE] ./efa_pipeline_validation/varimax_top_features.csv\n",
      "[SAVE] representatives*.csv/json/log -> ./efa_pipeline_validation\n",
      "[REP] Varimax＋一意化の代表（top1/因子）: ['num_INTJ_ratio', 'num_ADV_ratio', 'total_tree_height_CEJCminus_ratio', 'num_shuujoshi_ratio', 'num_sentences', 'w2Vsim_mean', 'num_kakujoshi_ratio', 'nmod_ratio', 'num_rentaishi_ratio']\n",
      "[FIT] 残差相関: max|r|=0.5776, Fro=3.6711\n",
      "[SAVE] ./efa_pipeline_validation/residual_corr_matrix.csv\n",
      "[SAVE] ./efa_pipeline_validation/top_residual_pairs_varimax.csv\n",
      "[SAVE] ./efa_pipeline_validation/bootstrap_varimax_rep_freq_top5.csv\n",
      "[SAVE] ./efa_pipeline_validation/mcsub_varimax_rep_freq_top5.csv\n",
      "[SAVE] ./efa_pipeline_validation/stability_varimax.png\n",
      "[SAVE] ./efa_pipeline_validation/summary_pipeline_validation.json\n",
      "\n",
      "=== 出力物 ===\n",
      "- efa_loadings_csv: ./efa_pipeline_validation/efa_loadings.csv\n",
      "- representatives_csv: ./efa_pipeline_validation/representatives.csv\n",
      "- representatives_log: ./efa_pipeline_validation/representatives_selection_log.csv\n",
      "- top10_varimax: ./efa_pipeline_validation/varimax_top_features.csv\n",
      "- bootstrap_rep_freq_top5: ./efa_pipeline_validation/bootstrap_varimax_rep_freq_top5.csv\n",
      "- mc_rep_freq_top5: ./efa_pipeline_validation/mcsub_varimax_rep_freq_top5.csv\n",
      "- stability_png: ./efa_pipeline_validation/stability_varimax.png\n",
      "- residual_matrix: ./efa_pipeline_validation/residual_corr_matrix.csv\n",
      "- residual_pairs: ./efa_pipeline_validation/top_residual_pairs_varimax.csv\n",
      "- pa_plot: ./efa_pipeline_validation/pa_corr.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# 実運用パイプライン準拠：EFA + Varimax + 代表選定 + PA(k決定) + 残差診断\n",
    "#                           ＋ 安定性検証（Bootstrap / MCサブサンプリング）\n",
    "# 必要: train_processed, pp_params['feature_cols'], ID_COL（無ければ 'ID'）\n",
    "# 出力: ./efa_pipeline_validation 以下にCSV/PNG/JSONを保存\n",
    "# =========================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import eigh, inv\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# -------------------------------\n",
    "# 設定\n",
    "# -------------------------------\n",
    "OUTDIR = \"./efa_pipeline_validation\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 乱数・反復\n",
    "SEED_FA  = 123\n",
    "SEED_PA  = 42\n",
    "SEED_BOOT= 456\n",
    "SEED_MC  = 789\n",
    "N_ITER_PA= 2000\n",
    "PCT_PA   = 99.0\n",
    "N_BOOT   = 2000\n",
    "N_MC     = 2000\n",
    "SS_RATIO = 0.70\n",
    "\n",
    "# 代表選定ポリシー（本文ポリシー）\n",
    "MIN_LOADING = 0.60\n",
    "MIN_GAP     = 0.20\n",
    "CROSS_MAX   = None   # ← 使わない（互換のため残置）\n",
    "MAX_ABS_R   = 0.80   # 代表間の相関一意化\n",
    "\n",
    "# -------------------------------\n",
    "# 入力チェック & ID平均\n",
    "# -------------------------------\n",
    "ID_COL = globals().get(\"ID_COL\", \"ID\")\n",
    "if 'train_processed' not in globals() or 'pp_params' not in globals():\n",
    "    raise ValueError(\"train_processed / pp_params が見つかりません。\")\n",
    "feat_cols = [c for c in pp_params.get(\"feature_cols\", []) if c in train_processed.columns]\n",
    "if len(feat_cols) == 0:\n",
    "    raise ValueError(\"pp_params['feature_cols'] が空、または列が見つかりません。\")\n",
    "if ID_COL not in train_processed.columns:\n",
    "    raise ValueError(f\"ID列 {ID_COL!r} が train_processed に存在しません。\")\n",
    "\n",
    "X_id_mean: pd.DataFrame = (\n",
    "    train_processed[[ID_COL] + feat_cols]\n",
    "    .dropna(subset=[ID_COL])\n",
    "    .groupby(ID_COL)[feat_cols].mean()\n",
    ")\n",
    "\n",
    "# ゼロ分散除外\n",
    "std0 = X_id_mean.std(ddof=1)\n",
    "zero_vars = std0.index[std0 == 0].tolist()\n",
    "if zero_vars:\n",
    "    X_id_mean = X_id_mean.drop(columns=zero_vars)\n",
    "    print(f\"[INFO] ゼロ分散列を除外: {zero_vars}\")\n",
    "\n",
    "print(f\"[INFO] ID平均: n_subjects={X_id_mean.shape[0]}, n_features={X_id_mean.shape[1]}\")\n",
    "\n",
    "# 標準化（列z）\n",
    "mu = X_id_mean.mean(axis=0)\n",
    "sd = X_id_mean.std(axis=0, ddof=1).replace(0, np.nan)\n",
    "Xz = (X_id_mean - mu) / sd\n",
    "Xz = Xz.fillna(0.0)\n",
    "\n",
    "# 参考：標準化前の分散トップ\n",
    "X_id_mean.var(ddof=1).sort_values(ascending=False).head(10)\\\n",
    "    .to_csv(os.path.join(OUTDIR, \"diag_top_variances_pre_standardize.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"diag_top_variances_pre_standardize.csv\"))\n",
    "\n",
    "# 相関行列\n",
    "R = Xz.corr().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "np.fill_diagonal(R.values, 1.0)\n",
    "\n",
    "# -------------------------------\n",
    "# Horn 型 Parallel Analysis（相関ベース）で k を決定\n",
    "# -------------------------------\n",
    "def _cov_eigvals_from_array(X_arr: np.ndarray) -> np.ndarray:\n",
    "    Xn = (X_arr - X_arr.mean(0, keepdims=True)) / (X_arr.std(0, keepdims=True) + 1e-12)\n",
    "    C = np.cov(Xn, rowvar=False)\n",
    "    C = (C + C.T) * 0.5\n",
    "    vals, _ = eigh(C)\n",
    "    return vals[::-1]\n",
    "\n",
    "def parallel_analysis_corr(X_df: pd.DataFrame, n_iter=N_ITER_PA, pct=PCT_PA, seed=SEED_PA) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    n, p = X_arr.shape\n",
    "    obs = _cov_eigvals_from_array(X_arr)\n",
    "    null_eigs = np.empty((n_iter, p), float)\n",
    "    for i in range(n_iter):\n",
    "        Z = rng.standard_normal(size=(n, p))\n",
    "        null_eigs[i] = _cov_eigvals_from_array(Z)\n",
    "    thr = np.percentile(null_eigs, pct, axis=0)\n",
    "    k = int(np.sum(obs > thr))\n",
    "    return {\"obs\": obs, \"thr\": thr, \"k\": k}\n",
    "\n",
    "pa = parallel_analysis_corr(Xz, n_iter=N_ITER_PA, pct=PCT_PA, seed=SEED_PA)\n",
    "k_final = max(1, pa[\"k\"])\n",
    "print(f\"[PA] 推奨k={k_final}  (pct={PCT_PA}, iter={N_ITER_PA})\")\n",
    "\n",
    "# 図保存\n",
    "def _save_pa_plot(obs, thr, k, path, title):\n",
    "    x = np.arange(1, len(obs)+1)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x, obs, marker='o', label='Observed eigs')\n",
    "    plt.plot(x, thr, marker='x', linestyle='--', label=f'Null {int(PCT_PA)}th pct')\n",
    "    if k>0:\n",
    "        plt.axvline(k, color='gray', linestyle=':')\n",
    "        plt.text(k+0.3, float(np.max(obs))*0.95, f'k={k}', ha='left', va='top')\n",
    "    plt.xlabel('Component index'); plt.ylabel('Eigenvalue')\n",
    "    plt.title(title); plt.legend(); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    print(\"[SAVE]\", path)\n",
    "\n",
    "_save_pa_plot(pa[\"obs\"], pa[\"thr\"], pa[\"k\"], os.path.join(OUTDIR, \"pa_corr.png\"),\n",
    "              \"Parallel Analysis (Correlation-based, Horn)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 因子分析（sklearn FA, EM）→ Varimax回転 → 負荷/上位/代表\n",
    "# -------------------------------\n",
    "def fit_fa_return_loadings_df(X_df: pd.DataFrame, n_factors: int, seed: int = SEED_FA) -> Tuple[pd.DataFrame, np.ndarray, FactorAnalysis]:\n",
    "    fa = FactorAnalysis(n_components=n_factors, random_state=seed)\n",
    "    W = fa.fit(X_df.values.astype(float)).components_.T  # (p x k)\n",
    "    cols = [f\"Factor{j+1}\" for j in range(n_factors)]\n",
    "    return pd.DataFrame(W, index=X_df.columns, columns=cols), fa.noise_variance_, fa\n",
    "\n",
    "def varimax(W: np.ndarray, gamma: float = 1.0, q: int = 20, tol: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    p, k = W.shape\n",
    "    Rm = np.eye(k); d = 0\n",
    "    for _ in range(q):\n",
    "        d0 = d\n",
    "        Lam = W @ Rm\n",
    "        u, s, vh = np.linalg.svd(\n",
    "            W.T @ (Lam**3 - (gamma/p) * Lam @ np.diag(np.sum(Lam**2, axis=0))),\n",
    "            full_matrices=False\n",
    "        )\n",
    "        Rm = u @ vh\n",
    "        d = np.sum(s)\n",
    "        if d0 != 0 and d/d0 < 1 + tol:\n",
    "            break\n",
    "    return W @ Rm, Rm  # (rotated, rotation matrix)\n",
    "\n",
    "def top_features_per_factor(load_df: pd.DataFrame, topn: int = 10) -> Dict[str, pd.DataFrame]:\n",
    "    res = {}\n",
    "    for c in load_df.columns:\n",
    "        s = load_df[c].abs().sort_values(ascending=False).head(topn)\n",
    "        res[c] = pd.DataFrame({\"feature\": s.index, \"abs_loading\": s.values})\n",
    "    return res\n",
    "\n",
    "def save_top_tables(tops: Dict[str, pd.DataFrame], prefix: str):\n",
    "    tbls = []\n",
    "    for j, (c, df) in enumerate(tops.items(), start=1):\n",
    "        df2 = df.copy()\n",
    "        df2.columns = [f\"Factor{j}_feature\", f\"Factor{j}_absload\"]\n",
    "        tbls.append(df2.reset_index(drop=True))\n",
    "    big = pd.concat(tbls, axis=1)\n",
    "    big.to_csv(os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(\"[SAVE]\", os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"))\n",
    "\n",
    "def select_reps_with_policy(load_df: pd.DataFrame, R_df: pd.DataFrame,\n",
    "                            min_loading: float = MIN_LOADING, min_gap: float = MIN_GAP,\n",
    "                            cross_max: float = CROSS_MAX, max_abs_r: float = MAX_ABS_R\n",
    "                           ) -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    代表選定ポリシー（改訂版）:\n",
    "      - |λ| ≥ min_loading\n",
    "      - 2番目に大きい因子負荷量との差（gap） ≥ min_gap\n",
    "      - 代表間の相関が一意化: 既選定代表との |r| ≤ max_abs_r\n",
    "    ※ cross_max（クロスローディング閾値）は一切用いない。\n",
    "    ※ 充足候補が無い因子は段階的に緩和（gap→loading→相関閾値）して必ず1本選ぶ。\n",
    "    \"\"\"\n",
    "    fac_cols = list(load_df.columns)\n",
    "    Labs = load_df.abs()\n",
    "    selected, rows = [], []\n",
    "    for f in fac_cols:\n",
    "        ser = Labs[f].sort_values(ascending=False)\n",
    "        chosen = None; decision = \"strict\"\n",
    "        # strict\n",
    "        for feat, lam in ser.items():\n",
    "            if feat in selected: \n",
    "                continue\n",
    "            others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "            max_other = float(others.max()) if len(others) else 0.0\n",
    "            gap = float(lam - max_other)\n",
    "            if (lam >= min_loading) and (gap >= min_gap) and \\\n",
    "               all(abs(R_df.loc[feat, r]) <= max_abs_r for r in selected):\n",
    "                chosen = (feat, float(load_df.loc[feat, f]), lam, max_other, gap)\n",
    "                break\n",
    "        # relax stages（クロスは使わない）\n",
    "        stages = [\n",
    "            (\"relax_gap\",   (min_loading, 0.10, max_abs_r)),\n",
    "            (\"relax_load\",  (0.00,        0.00, max_abs_r)),\n",
    "            (\"relax_corr\",  (0.00,        0.00, 0.95))\n",
    "        ]\n",
    "        if chosen is None:\n",
    "            for name, (ml, mg, rr) in stages:\n",
    "                for feat, lam in ser.items():\n",
    "                    if feat in selected: \n",
    "                        continue\n",
    "                    others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "                    max_other = float(others.max()) if len(others) else 0.0\n",
    "                    gap = float(lam - max_other)\n",
    "                    if (lam >= ml) and (gap >= mg) and \\\n",
    "                       all(abs(R_df.loc[feat, r]) <= rr for r in selected):\n",
    "                        chosen = (feat, float(load_df.loc[feat, f]), lam, max_other, gap)\n",
    "                        decision = name\n",
    "                        break\n",
    "                if chosen is not None:\n",
    "                    break\n",
    "        if chosen is None:\n",
    "            # 最終手段：未使用の最大\n",
    "            feat = ser.index[0]\n",
    "            lam = float(ser.iloc[0]); lam_signed = float(load_df.loc[feat, f])\n",
    "            others = Labs.loc[feat, [c for c in fac_cols if c != f]]\n",
    "            max_other = float(others.max()) if len(others) else 0.0\n",
    "            gap = float(lam - max_other)\n",
    "            chosen = (feat, lam_signed, lam, max_other, gap)\n",
    "            decision = \"fallback_max\"\n",
    "        feat, lam_signed, lam_abs, max_other, gap = chosen\n",
    "        selected.append(feat)\n",
    "        rows.append({\n",
    "            \"factor\": f, \"feature\": feat, \"loading\": lam_signed, \"abs_loading\": lam_abs,\n",
    "            \"second_best_abs\": max_other, \"gap_abs\": gap, \"decision\": decision\n",
    "        })\n",
    "    log_df = pd.DataFrame(rows)\n",
    "    return selected, log_df\n",
    "\n",
    "# --- FA fit & rotate ---\n",
    "load_raw, psi_raw, fa_model = fit_fa_return_loadings_df(Xz, k_final, seed=SEED_FA)\n",
    "W_rot, _ = varimax(load_raw.values.copy())\n",
    "load_varimax = pd.DataFrame(W_rot, index=load_raw.index, columns=load_raw.columns)\n",
    "\n",
    "# 保存（efa_loadings.csv も出す）\n",
    "load_raw.to_csv(os.path.join(OUTDIR, \"loadings_raw.csv\"), encoding=\"utf-8\")\n",
    "load_varimax.to_csv(os.path.join(OUTDIR, \"loadings_varimax.csv\"), encoding=\"utf-8\")\n",
    "load_varimax.to_csv(os.path.join(OUTDIR, \"efa_loadings.csv\"), encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"loadings_raw.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"loadings_varimax.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"efa_loadings.csv\"))\n",
    "\n",
    "# 各因子 Top10\n",
    "tops_raw = top_features_per_factor(load_raw, topn=10)\n",
    "tops_rot = top_features_per_factor(load_varimax, topn=10)\n",
    "def _save_top(prefix, tops):\n",
    "    tbls = []\n",
    "    for j, (c, df) in enumerate(tops.items(), start=1):\n",
    "        d = df.copy(); d.columns=[f\"Factor{j}_feature\", f\"Factor{j}_absload\"]\n",
    "        tbls.append(d.reset_index(drop=True))\n",
    "    big = pd.concat(tbls, axis=1)\n",
    "    big.to_csv(os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(\"[SAVE]\", os.path.join(OUTDIR, f\"{prefix}_top_features.csv\"))\n",
    "_save_top(\"raw\", tops_raw)\n",
    "_save_top(\"varimax\", tops_rot)\n",
    "\n",
    "# 代表選定（本文ポリシー：クロス不使用）\n",
    "reps, rep_log = select_reps_with_policy(load_varimax, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "rep_log.to_csv(os.path.join(OUTDIR, \"representatives_selection_log.csv\"), index=False, encoding=\"utf-8\")\n",
    "pd.DataFrame({\"feature\": reps, \"factor\": [f\"Factor{i+1}\" for i in range(len(reps))]})\\\n",
    "  .to_csv(os.path.join(OUTDIR, \"representatives.csv\"), index=False, encoding=\"utf-8\")\n",
    "with open(os.path.join(OUTDIR, \"representatives.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"representatives\": reps}, f, ensure_ascii=False, indent=2)\n",
    "print(\"[SAVE] representatives*.csv/json/log ->\", OUTDIR)\n",
    "print(\"[REP] Varimax＋一意化の代表（top1/因子）:\", reps)\n",
    "\n",
    "# -------------------------------\n",
    "# 適合度診断（回転後負荷での再現相関）\n",
    "# -------------------------------\n",
    "def reproduced_corr_from_loadings(W: np.ndarray, psi: np.ndarray) -> np.ndarray:\n",
    "    R_hat = W @ W.T + np.diag(psi)\n",
    "    # 対角=1に補正（標準化空間）\n",
    "    for i in range(R_hat.shape[0]):\n",
    "        R_hat[i, i] = 1.0\n",
    "    return R_hat\n",
    "\n",
    "R_hat = reproduced_corr_from_loadings(load_varimax.values, psi_raw)\n",
    "resid = R.values - R_hat\n",
    "max_abs_resid = float(np.max(np.abs(resid)))\n",
    "frob = float(np.linalg.norm(resid, \"fro\"))\n",
    "np.savetxt(os.path.join(OUTDIR, \"residual_corr_matrix.csv\"), resid, delimiter=\",\")\n",
    "print(f\"[FIT] 残差相関: max|r|={max_abs_resid:.4f}, Fro={frob:.4f}\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"residual_corr_matrix.csv\"))\n",
    "\n",
    "# 上位残差ペア（冗長候補の確認用）\n",
    "p = len(load_varimax.index)\n",
    "tri_idx = [(i,j) for i in range(p) for j in range(i+1, p)]\n",
    "tri_vals = [abs(resid[i,j]) for (i,j) in tri_idx]\n",
    "order = np.argsort(tri_vals)[::-1][:30]\n",
    "rows = []\n",
    "feat_names = list(load_varimax.index)\n",
    "for idx in order:\n",
    "    i,j = tri_idx[idx]\n",
    "    rows.append({\n",
    "        \"feature_i\": feat_names[i], \"feature_j\": feat_names[j],\n",
    "        \"abs_residual_corr\": float(abs(resid[i,j])),\n",
    "        \"residual_corr\": float(resid[i,j]),\n",
    "        \"raw_corr\": float(R.values[i,j])\n",
    "    })\n",
    "pd.DataFrame(rows).to_csv(os.path.join(OUTDIR, \"top_residual_pairs_varimax.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"top_residual_pairs_varimax.csv\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 安定性検証（Bootstrap / MC）※パイプラインと同一手順を厳守\n",
    "# -------------------------------\n",
    "def orthogonal_procrustes_align(W_ref: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
    "    U, _, Vt = np.linalg.svd(W.T @ W_ref, full_matrices=False)\n",
    "    Q = U @ Vt\n",
    "    return W @ Q\n",
    "\n",
    "def tucker_congruence(W1: np.ndarray, W2: np.ndarray) -> float:\n",
    "    num = (W1 * W2).sum(axis=0)\n",
    "    den = np.sqrt((W1**2).sum(axis=0) * (W2**2).sum(axis=0)) + 1e-12\n",
    "    return float(np.mean(num / den))\n",
    "\n",
    "def _rep_freq_to_df(rep_counts: List[Dict[str,int]], topn: int = 5) -> pd.DataFrame:\n",
    "    tables = []\n",
    "    for j, cnt in enumerate(rep_counts, start=1):\n",
    "        total = sum(cnt.values()) if len(cnt) else 1\n",
    "        freq = {k: v/total for k,v in cnt.items()}\n",
    "        top = sorted(freq.items(), key=lambda kv: -kv[1])[:topn]\n",
    "        tables.append(pd.DataFrame(top, columns=[f\"Factor{j}_feature\", \"freq\"]).reset_index(drop=True))\n",
    "    return pd.concat(tables, axis=1)\n",
    "\n",
    "def stability_bootstrap(X_df: pd.DataFrame, k: int, n_boot=N_BOOT, seed=SEED_BOOT) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 参照（Full-data）：FA→Varimax→代表（本文ポリシー）\n",
    "    load0, psi0, _ = fit_fa_return_loadings_df(X_df, k, seed=rng.integers(1e9))\n",
    "    Wref, _ = varimax(load0.values)\n",
    "    load_ref = pd.DataFrame(Wref, index=load0.index, columns=load0.columns)\n",
    "    reps_ref, _ = select_reps_with_policy(load_ref, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "\n",
    "    congr = []; rep_counts = [dict() for _ in range(k)]\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    for _ in range(n_boot):\n",
    "        Xb = resample(X_arr, replace=True, n_samples=X_arr.shape[0], random_state=rng.integers(1e9))\n",
    "        load_b, _, _ = fit_fa_return_loadings_df(pd.DataFrame(Xb, columns=X_df.columns), k, seed=rng.integers(1e9))\n",
    "        Wb, _ = varimax(load_b.values)\n",
    "        Wb_al = orthogonal_procrustes_align(Wref, Wb)\n",
    "        congr.append(tucker_congruence(Wref, Wb_al))\n",
    "        reps_b, _ = select_reps_with_policy(pd.DataFrame(Wb_al, index=load_b.index, columns=load_b.columns),\n",
    "                                            R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "        for j, name in enumerate(reps_b):\n",
    "            rep_counts[j][name] = rep_counts[j].get(name, 0) + 1\n",
    "\n",
    "    rep_top5 = _rep_freq_to_df(rep_counts, topn=5)\n",
    "    return {\n",
    "        \"tucker_mean\": float(np.mean(congr)),\n",
    "        \"tucker_sd\":   float(np.std(congr, ddof=1)),\n",
    "        \"rep_counts\":  rep_counts,\n",
    "        \"rep_top5\":    rep_top5,\n",
    "        \"reps_ref\":    reps_ref\n",
    "    }\n",
    "\n",
    "def stability_mc_subsample(X_df: pd.DataFrame, k: int, n_iter=N_MC, ratio=SS_RATIO, seed=SEED_MC) -> Dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 参照\n",
    "    load0, psi0, _ = fit_fa_return_loadings_df(X_df, k, seed=rng.integers(1e9))\n",
    "    Wref, _ = varimax(load0.values)\n",
    "    load_ref = pd.DataFrame(Wref, index=load0.index, columns=load0.columns)\n",
    "    reps_ref, _ = select_reps_with_policy(load_ref, R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "\n",
    "    congr = []; rep_counts = [dict() for _ in range(k)]\n",
    "    X_arr = X_df.values.astype(float)\n",
    "    n = X_arr.shape[0]; ss = max(2, int(round(n * ratio)))\n",
    "    for _ in range(n_iter):\n",
    "        idx = rng.choice(n, size=ss, replace=False)\n",
    "        Xi = X_arr[idx]\n",
    "        load_i, _, _ = fit_fa_return_loadings_df(pd.DataFrame(Xi, columns=X_df.columns), k, seed=rng.integers(1e9))\n",
    "        Wi, _ = varimax(load_i.values)\n",
    "        Wi_al = orthogonal_procrustes_align(Wref, Wi)\n",
    "        congr.append(tucker_congruence(Wref, Wi_al))\n",
    "        reps_i, _ = select_reps_with_policy(pd.DataFrame(Wi_al, index=load_i.index, columns=load_i.columns),\n",
    "                                            R, MIN_LOADING, MIN_GAP, CROSS_MAX, MAX_ABS_R)\n",
    "        for j, name in enumerate(reps_i):\n",
    "            rep_counts[j][name] = rep_counts[j].get(name, 0) + 1\n",
    "\n",
    "    rep_top5 = _rep_freq_to_df(rep_counts, topn=5)\n",
    "    return {\n",
    "        \"tucker_mean\": float(np.mean(congr)),\n",
    "        \"tucker_sd\":   float(np.std(congr, ddof=1)),\n",
    "        \"rep_counts\":  rep_counts,\n",
    "        \"rep_top5\":    rep_top5,\n",
    "        \"reps_ref\":    reps_ref\n",
    "    }\n",
    "\n",
    "# 実行\n",
    "boot = stability_bootstrap(Xz, k_final, n_boot=N_BOOT, seed=SEED_BOOT)\n",
    "mc   = stability_mc_subsample(Xz, k_final, n_iter=N_MC, ratio=SS_RATIO, seed=SEED_MC)\n",
    "\n",
    "# 保存（頻度Top5、棒グラフ）\n",
    "boot[\"rep_top5\"].to_csv(os.path.join(OUTDIR, \"bootstrap_varimax_rep_freq_top5.csv\"), index=False, encoding=\"utf-8\")\n",
    "mc[\"rep_top5\"].to_csv(os.path.join(OUTDIR, \"mcsub_varimax_rep_freq_top5.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"bootstrap_varimax_rep_freq_top5.csv\"))\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"mcsub_varimax_rep_freq_top5.csv\"))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([0,1], [boot[\"tucker_mean\"], mc[\"tucker_mean\"]])\n",
    "plt.xticks([0,1], ['Bootstrap', 'MC-Subsample'])\n",
    "plt.ylabel(\"Tucker's congruence (mean)\")\n",
    "plt.title('Stability: varimax (pipeline-conformant)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"stability_varimax.png\"), dpi=150); plt.close()\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"stability_varimax.png\"))\n",
    "\n",
    "# -------------------------------\n",
    "# サマリ保存\n",
    "# -------------------------------\n",
    "summary = {\n",
    "    \"n_subjects\": int(Xz.shape[0]),\n",
    "    \"n_features\": int(Xz.shape[1]),\n",
    "    \"k_final\": int(k_final),\n",
    "    \"pa\": {\"pct\": PCT_PA, \"n_iter\": N_ITER_PA, \"k\": int(pa[\"k\"])},\n",
    "    \"fa\": {\"seed\": SEED_FA},\n",
    "    \"representatives\": reps,\n",
    "    \"fit\": {\"max_abs_resid\": max_abs_resid, \"frob\": frob},\n",
    "    \"stability\": {\n",
    "        \"bootstrap\": {\"tucker_mean\": boot[\"tucker_mean\"], \"tucker_sd\": boot[\"tucker_sd\"]},\n",
    "        \"mc\":        {\"tucker_mean\": mc[\"tucker_mean\"],   \"tucker_sd\": mc[\"tucker_sd\"]}\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"min_loading\": MIN_LOADING,\n",
    "        \"min_gap\": MIN_GAP,\n",
    "        \"max_abs_r\": MAX_ABS_R,\n",
    "        \"cross_max_used\": False\n",
    "    },\n",
    "    \"seeds\": {\"PA\": SEED_PA, \"FA\": SEED_FA, \"BOOT\": SEED_BOOT, \"MC\": SEED_MC},\n",
    "    \"files\": {\n",
    "        \"efa_loadings_csv\": \"efa_loadings.csv\",\n",
    "        \"representatives_csv\": \"representatives.csv\",\n",
    "        \"representatives_log\": \"representatives_selection_log.csv\",\n",
    "        \"top10_varimax\": \"varimax_top_features.csv\",\n",
    "        \"bootstrap_rep_freq_top5\": \"bootstrap_varimax_rep_freq_top5.csv\",\n",
    "        \"mc_rep_freq_top5\": \"mcsub_varimax_rep_freq_top5.csv\",\n",
    "        \"stability_png\": \"stability_varimax.png\",\n",
    "        \"residual_matrix\": \"residual_corr_matrix.csv\",\n",
    "        \"residual_pairs\": \"top_residual_pairs_varimax.csv\",\n",
    "        \"pa_plot\": \"pa_corr.png\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUTDIR, \"summary_pipeline_validation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(\"[SAVE]\", os.path.join(OUTDIR, \"summary_pipeline_validation.json\"))\n",
    "\n",
    "print(\"\\n=== 出力物 ===\")\n",
    "for k,v in summary[\"files\"].items():\n",
    "    print(f\"- {k}: {os.path.join(OUTDIR, v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0ef92-913e-423d-8453-0ccdb9750ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp311)",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
